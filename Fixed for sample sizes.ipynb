{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4_data[0]: id\n",
      "e4_data[1]: trial\n",
      "e4_data[2]: EDA_PPT\n",
      "e4_data[3]: HR_PPT\n",
      "e4_data[4]: TEMP_PPT\n",
      "e4_data[5]: BVP_PPT\n",
      "e4_data[6]: ACC_PPT\n",
      "e4_data[7]: IBI_PPT\n",
      "e4_data[8]: EDA_FREQ_PPT\n",
      "e4_data[9]: EDA_AMP_PPT\n",
      "change_data[0]: id\n",
      "change_data[1]: Brief fear of Negative Evaluation\n",
      "change_data[2]: CAI Trait Dyadic Score\n",
      "change_data[3]: CAI Trait Full Score\n",
      "change_data[4]: CAI Trait Public Speaking Score\n",
      "change_data[5]: CAI Trait Small group Score\n",
      "change_data[6]: STAI Trait Score\n",
      "change_data[7]: CAI State Score\n",
      "change_data[8]: STAI State Score\n",
      "audio_data[0]: id\n",
      "audio_data[1]: trial\n",
      "audio_data[2]: pcm_RMSenergy_sma_amean\n",
      "audio_data[3]: pcm_fftMag_mfcc_sma[1]_amean\n",
      "audio_data[4]: pcm_fftMag_mfcc_sma[2]_amean\n",
      "audio_data[5]: pcm_fftMag_mfcc_sma[3]_amean\n",
      "audio_data[6]: pcm_fftMag_mfcc_sma[4]_amean\n",
      "audio_data[7]: pcm_fftMag_mfcc_sma[5]_amean\n",
      "audio_data[8]: pcm_fftMag_mfcc_sma[6]_amean\n",
      "audio_data[9]: pcm_fftMag_mfcc_sma[7]_amean\n",
      "audio_data[10]: pcm_fftMag_mfcc_sma[8]_amean\n",
      "audio_data[11]: pcm_fftMag_mfcc_sma[9]_amean\n",
      "audio_data[12]: pcm_fftMag_mfcc_sma[10]_amean\n",
      "audio_data[13]: pcm_fftMag_mfcc_sma[11]_amean\n",
      "audio_data[14]: pcm_fftMag_mfcc_sma[12]_amean\n",
      "audio_data[15]: pcm_zcr_sma_amean\n",
      "audio_data[16]: voiceProb_sma_amean\n",
      "audio_data[17]: F0_sma_amean\n",
      "audio_data[18]: #pause\n",
      "audio_data[19]: pause_frequency\n",
      "audio_data[20]: pause_interval\n",
      "audio_data[21]: mean\n",
      "audio_data[22]: percent\n",
      "audio_data[23]: jitterLocal_sma_amean\n",
      "audio_data[24]: jitterDDP_sma_amean\n",
      "audio_data[25]: shimmerLocal_sma_amean\n",
      "demo_data[0]: id\n",
      "demo_data[1]: Age\n",
      "demo_data[2]: Gender\n",
      "demo_data[3]: Lang\n",
      "demo_data[4]: college\n",
      "demo_data[5]: presentation\n",
      "demo_data[6]: ethnicity\n",
      "demo_data[7]: presentation_3_months\n",
      "demo_data[8]: highest_education\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "#############################\n",
    "# Import and combine all data\n",
    "#############################\n",
    "\n",
    "\n",
    "#DATA_PATH = \"/Users/mvonebers/HUBBS-Lab/data/\"\n",
    "DATA_PATH = \"/home/maggie/HUBBS-Lab/data/\"\n",
    "\n",
    "e4_data = pd.read_excel(DATA_PATH + \"E4_TEST.xlsx\")\n",
    "change_data = pd.read_excel(DATA_PATH + \"normalized_change.xlsx\")\n",
    "audio_data = pd.read_excel(DATA_PATH + \"audio_TEST.xlsx\")\n",
    "demo_data = pd.read_csv(DATA_PATH + \"Demographics Information.csv\")\n",
    "\n",
    "\n",
    "# Break apart the ID column into \"person\" and \"trial\"\n",
    "def clean_id(data):\n",
    "    data.insert(0, \"person\", [0] * data.shape[0])\n",
    "    data.insert(1, \"trial\", [0] * data.shape[0])\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        data.at[i, \"person\"] = int(data.at[i, \"id\"][7:])\n",
    "        data.at[i, \"trial\"] = int(data.at[i, \"id\"][5])\n",
    "    \n",
    "    data = data.drop(columns=['id'])\n",
    "    data = data.rename(columns={\"person\": \"id\"})\n",
    "    return data\n",
    "\n",
    "    \n",
    "e4_data = clean_id(e4_data)\n",
    "audio_data = clean_id(audio_data)\n",
    "\n",
    "# Normalize and print out indices for ease of reference\n",
    "for dframe, name in zip([e4_data, change_data, audio_data, demo_data], ['e4_data', 'change_data', 'audio_data', 'demo_data']):\n",
    "    for i, col in zip(range(len(dframe.columns.to_list())), dframe.columns.to_list()):\n",
    "        if not (col == 'id' or col == 'trial'):\n",
    "            dframe[col] = dframe[col]/dframe[col].max()\n",
    "        print(name + '[' + str(i) + ']: ' + str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Get slopes from linear regression of the 8 trials for each ID\n",
    "###############################################################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def get_slopes(data, start, end):\n",
    "    iterations = int(data.shape[0]/8)\n",
    "    y0 = data['trial'].to_numpy(copy=True)\n",
    "\n",
    "    slopes = pd.DataFrame(np.zeros((iterations, data.shape[1])), columns=data.columns)\n",
    "    missing_trials = pd.DataFrame(np.zeros((iterations, data.shape[1])), columns=data.columns)\n",
    "    missing_trials = missing_trials.drop([\"trial\"], axis=1)\n",
    "    slopes = slopes.drop([\"trial\"], axis=1)\n",
    "\n",
    "    for col in range(2, data.shape[1]):\n",
    "        x1 = data[data.columns[col]]\n",
    "        y0 = list(range(start, end + 1))\n",
    "        for row in range(iterations):\n",
    "            missing = 0\n",
    "            x0 = x1[ (row * 8) + start - 1 : (row * 8) + end ].to_numpy()\n",
    "            x = np.array([])\n",
    "            y = np.array([])\n",
    "            \n",
    "            slopes.iloc[row, 0] = data.iloc[row * 8, 0]\n",
    "\n",
    "            for i in range(len(x0)):  # remove NaN from data\n",
    "                if math.isnan(x0[i]) or math.isnan(y0[i]):\n",
    "                    missing += 1\n",
    "                elif x0[i] == 0: #and not data.columns[col] == '#pause':\n",
    "                    missing += 1\n",
    "                else:\n",
    "                    x = np.append(x, x0[i])\n",
    "                    y = np.append(y, y0[i])\n",
    "                    \n",
    "            missing_trials.iloc[row, col - 1] = missing\n",
    "                    \n",
    "            try:\n",
    "                reg = LinearRegression().fit(y.reshape(-1,1),x)\n",
    "                slopes.iloc[row, col - 1] = reg.coef_\n",
    "            except:\n",
    "                 slopes.iloc[row, col - 1] = 0\n",
    "                    \n",
    "    slopes.replace(0, np.NaN, inplace=True)\n",
    "    missing_trials['id'] = slopes['id']\n",
    "    \n",
    "    return slopes, missing_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e4_slopes, e4_missing = get_slopes(e4_data, 1, 8)\n",
    "audio_slopes, audio_missing = get_slopes(audio_data, 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1593750000355078"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpause = np.array([audio_data['#pause'].to_numpy()])\n",
    "freq = np.array([audio_data['pause_frequency'].to_numpy()])\n",
    "audio_data['pause_frequency'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_pair(x01, x02):\n",
    "    x1 = x01.copy()\n",
    "    x2 = x02.copy()\n",
    "    set01 = set(x01['id'].to_list())\n",
    "    set02 = set(x02['id'].to_list())\n",
    "    ids = set01.symmetric_difference(set02)\n",
    "    print(ids)\n",
    "    for i in ids:\n",
    "        x1 = x1[x1.id != i]\n",
    "        x2 = x2[x2.id != i]\n",
    "        \n",
    "    final = x1.merge(x2, how=\"right\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{65.0, 66.0, 16.0, 38.0, 46.0, 53.0, 58.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EDA_PPT</th>\n",
       "      <th>IBI_PPT</th>\n",
       "      <th>#pause</th>\n",
       "      <th>pause_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.005964</td>\n",
       "      <td>-0.004762</td>\n",
       "      <td>-0.003433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.006102</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.015774</td>\n",
       "      <td>0.020945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>-0.021424</td>\n",
       "      <td>0.039881</td>\n",
       "      <td>0.051226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.017983</td>\n",
       "      <td>0.028971</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.003823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>-0.031028</td>\n",
       "      <td>-0.012828</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>-0.007599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.013448</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>-0.005681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32.0</td>\n",
       "      <td>-0.020056</td>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>-0.005396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35.0</td>\n",
       "      <td>-0.086381</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37.0</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.050482</td>\n",
       "      <td>-0.003814</td>\n",
       "      <td>-0.003830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.000784</td>\n",
       "      <td>-0.042194</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.029613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>42.0</td>\n",
       "      <td>-0.052572</td>\n",
       "      <td>-0.047336</td>\n",
       "      <td>-0.001488</td>\n",
       "      <td>-0.005104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44.0</td>\n",
       "      <td>-0.000417</td>\n",
       "      <td>0.014518</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.028795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>47.0</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.029464</td>\n",
       "      <td>0.032432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.030980</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.005466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>51.0</td>\n",
       "      <td>-0.001345</td>\n",
       "      <td>-0.003339</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>0.001619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>61.0</td>\n",
       "      <td>-0.061296</td>\n",
       "      <td>0.037874</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>-0.012423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.007549</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>-0.007738</td>\n",
       "      <td>-0.017087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>-0.004841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>73.0</td>\n",
       "      <td>-0.039827</td>\n",
       "      <td>-0.011449</td>\n",
       "      <td>-0.003125</td>\n",
       "      <td>-0.024063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   EDA_PPT   IBI_PPT    #pause  pause_frequency\n",
       "0    4.0  0.000002 -0.005964 -0.004762        -0.003433\n",
       "1    5.0 -0.006102  0.000766  0.015774         0.020945\n",
       "2    8.0  0.002106 -0.021424  0.039881         0.051226\n",
       "3   20.0 -0.017983  0.028971  0.000205         0.003823\n",
       "4   21.0 -0.031028 -0.012828 -0.002530        -0.007599\n",
       "5   23.0  0.001176  0.013448  0.000893        -0.005681\n",
       "6   32.0 -0.020056  0.023953  0.013542        -0.005396\n",
       "7   35.0 -0.086381 -0.000420       NaN         0.012492\n",
       "8   37.0 -0.000264 -0.050482 -0.003814        -0.003830\n",
       "9   41.0 -0.000784 -0.042194 -0.034821        -0.029613\n",
       "10  42.0 -0.052572 -0.047336 -0.001488        -0.005104\n",
       "11  44.0 -0.000417  0.014518  0.022321         0.028795\n",
       "12  47.0  0.000378  0.012001  0.029464         0.032432\n",
       "13  50.0 -0.030980  0.000766 -0.004167        -0.005466\n",
       "14  51.0 -0.001345 -0.003339 -0.008333         0.001619\n",
       "15  61.0 -0.061296  0.037874  0.006994        -0.012423\n",
       "16  62.0 -0.007549 -0.000150 -0.007738        -0.017087\n",
       "17  71.0  0.000347  0.009901 -0.005952        -0.004841\n",
       "18  73.0 -0.039827 -0.011449 -0.003125        -0.024063"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-96-bf1fcbede1f3>, line 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-96-bf1fcbede1f3>\"\u001b[0;36m, line \u001b[0;32m100\u001b[0m\n\u001b[0;31m    def generate_combos(e4_slope_data, audio_slope_data):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "y_columns = ['CAI State Score', 'CAI Trait Full Score', 'STAI Trait Score']\n",
    "\n",
    "\n",
    "def get_combo_predictions(X0, slope_data):\n",
    "    corrs = []\n",
    "    ps = []\n",
    "    for y_col, y_i in zip(y_columns, range(len(y_columns))):\n",
    "        y0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        X = np.array([X0[0]])\n",
    "        y = np.array(y0[0])\n",
    "        \n",
    "        for i in range(1, len(X0)):  # remove NaN from data\n",
    "            is_nan = False\n",
    "            for x in X0[i]:\n",
    "                if math.isnan(x):\n",
    "                    is_nan = True\n",
    "                    break\n",
    "            if not math.isnan(y0[i]) and not is_nan:\n",
    "                X = np.append(X, [X0[i]], axis=0)\n",
    "                y = np.append(y, y0[i])\n",
    "        \n",
    "        \n",
    "        folds = min(10, len(X))\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(folds, shuffle=True, random_state=42)\n",
    "        predicted_vals0 = cross_val_predict(model, X, y, cv=cv)\n",
    "        actual_vals0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        #TODO: why this for actual_vals0 isntead of the sanitized y we already got?\n",
    "        predicted_vals = []\n",
    "        actual_vals = []\n",
    "        \n",
    "\n",
    "        for j in range(len(predicted_vals0)):\n",
    "            if not math.isnan(predicted_vals0[j]) and not math.isnan(actual_vals0[j]):\n",
    "                predicted_vals.append(predicted_vals0[j])\n",
    "                actual_vals.append(actual_vals0[j])\n",
    "\n",
    "        correlation, pval = pearsonr(predicted_vals, actual_vals)\n",
    "        #to_print = str(correlation) + str(pval)\n",
    "        #if pval < 0.15: \n",
    "        #    if not folds == 10:\n",
    "        #        print(\"With # KFolds\", folds)\n",
    "        #    print(\"{0}:\\t\\t{1}\\t\\t{2}\\t{3}\\t\\t{4}\".format(y_col, correlation, pval, \"Rows:\", len(y)))\n",
    "        corrs.append(correlation)\n",
    "        ps.append(pval)\n",
    "        \n",
    "    return corrs, ps\n",
    "\n",
    "def get_predictions(X0, slope_data):\n",
    "    corrs = []\n",
    "    ps = []\n",
    "    num_samples = []\n",
    "    for y_col, y_i in zip(y_columns, range(len(y_columns))):\n",
    "        y0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        X = np.array(X0[0])\n",
    "        y = np.array(y0[0])\n",
    "        \n",
    "        for i in range(1,len(X0)):  # remove NaN from data\n",
    "            if math.isnan(y0[i]) or math.isnan(X0[i]):\n",
    "                continue\n",
    "            else:\n",
    "                X = np.append(X, X0[i])\n",
    "                y = np.append(y, y0[i])\n",
    "            \n",
    "        X = X.reshape(-1, 1)\n",
    "        folds = min(10, len(X))\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(folds, shuffle=True, random_state=42)\n",
    "        predicted_vals0 = cross_val_predict(model, X, y, cv=cv)\n",
    "        actual_vals0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        predicted_vals = []\n",
    "        actual_vals = []\n",
    "        \n",
    "\n",
    "        for j in range(len(predicted_vals0)):\n",
    "            if not math.isnan(predicted_vals0[j]) and not math.isnan(actual_vals0[j]):\n",
    "                predicted_vals.append(predicted_vals0[j])\n",
    "                actual_vals.append(actual_vals0[j])\n",
    "\n",
    "        correlation, pval = pearsonr(predicted_vals, actual_vals)\n",
    "        corrs.append(correlation)\n",
    "        ps.append(pval)\n",
    "        num_samples.append(len(actual_vals))\n",
    "        \n",
    "    return corrs, ps, num_samples\n",
    "            \n",
    "      \n",
    "def generate_combos(e4_slope_data, audio_slope_data):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_num_samples = []\n",
    "    group_titles = []\n",
    "    \n",
    "    demo_columns = ['Age', 'Lang', 'ethnicity', 'highest_education']\n",
    "    \n",
    "    demos = demo_data[demo_columns]\n",
    "    all_bios = e4_slope_data[e4_slope_data.columns[1:10]]\n",
    "    mfccs = audio_slope_data[[['id'] + audio_slope_data.columns[3:15]]]\n",
    "    pauses = audio_slope_data[[['id'] + audio_slope_data.columns[18:21]]]\n",
    "    jitter_shimmer = audio_slope_data[[['id'] + audio_slope_data.columns[23:25]]]\n",
    "    ppts = e4_slope_data[['id', 'HR_PPT', 'EDA_FREQ_PPT']]\n",
    "    \n",
    "    group_titles.append('all demos')\n",
    "    X0_demo = demo_data[demo_columns]\n",
    "    c, p, num_samples = get_combo_predictions(X0_demo, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append(\"bios + demos\")\n",
    "    X0 = prune_pair(all_bios, demos)\n",
    "    X0.drop(['id'])\n",
    "    X0 = X0.to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('mfcc1-12 + demos')\n",
    "    X0 = prune_pair(mfccs, demos)\n",
    "    X0.drop(['id'])\n",
    "    X0 = X0.to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    X0 = mfccs.to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    X0 = pauses.to_numpy(copy=True)\n",
    "    c, p, num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    #for col in slope_data.columns[17:20].to_list():\n",
    "    #    group_titles.append(col)\n",
    "    #    X0 = slope_data[col].to_numpy(copy=True)\n",
    "    #    c,p,num_samples = get_predictions(X0, slope_data)\n",
    "    #    group_c.append(c)\n",
    "    #    group_p.append(p)\n",
    "    #    group_num_samples.append(num_samples)\n",
    "\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    X0 = prune_pair(mfccs, pauses)\n",
    "    X0.drop(['id'])\n",
    "    X0 = X0.to_numpy(copy=True)\n",
    "    c, p, num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    X0 = jitter_shimmer.to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    # Just the two jitter variables (they performed better)\n",
    "    #group_titles.append('jitterDDP_sma_amean, jitterLocal_sma_amean')\n",
    "    #columns = ['jitterDDP_sma_amean', 'jitterLocal_sma_amean']\n",
    "    #X0_1 = slope_data[columns].to_numpy(copy=True)\n",
    "    #c, p, num_samples = get_combo_predictions(X0_1, slope_data)\n",
    "    #group_c.append(c)\n",
    "    #group_p.append(p)\n",
    "    #group_num_samples.append(num_samples)\n",
    "    \n",
    "    # Jitter, shimmer individually\n",
    "    #for col in slope_data.columns[22:25].to_list():\n",
    "    #    group_titles.append(col)\n",
    "    #    X0 = slope_data[col].to_numpy(copy=True)\n",
    "    #    c,p,num_samples = get_predictions(X0, slope_data)\n",
    "    #    group_c.append(c)\n",
    "    #    group_p.append(p)\n",
    "    #    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('jitter, shimmer, pauses')\n",
    "    X0_jitter = prune_pair(jitter_shimmer, pauses)\n",
    "    X0.drop(['id'])\n",
    "    X0 = X0.to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('mfccs + jitter, shimmer')\n",
    "    X0 = prune_pair(mfccs, jitter_shimmer)\n",
    "    X0.drop(['id'])\n",
    "    X0 = X0.to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('mfccs + jitter shimmer + pauses')\n",
    "    temp = prune_pair(mfccs, jitter_shimmer)\n",
    "    X0 = prune_pair(temp, pauses)\n",
    "    X0.drop(['id'])\n",
    "    X0 = X0.to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    X01 = all_bios.copy().drop(['id'])\n",
    "    X0 = X01.to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    #group_titles.append('HR_PPT, EDA_FREQ_PPT')\n",
    "    #columns = ['HR_PPT', 'EDA_FREQ_PPT']\n",
    "    #X0_0 = slope_data[columns].to_numpy(copy=True)\n",
    "    #c, p, num_samples = get_combo_predictions(X0_0, slope_data)\n",
    "    #group_c.append(c)\n",
    "    #group_p.append(p)\n",
    "    #group_num_samples.append(num_samples)\n",
    "    \n",
    "    for col in slope_data.columns[25:33].to_list():\n",
    "        group_titles.append(col)\n",
    "        X0 = slope_data[col].to_numpy(copy=True)\n",
    "        c,p,num_samples = get_predictions(X0, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "        group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('bio + jitter, shimmer')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[22:25].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('bio + jitter, shimmer + pauses')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[22:25].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('bio + pauses')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[2:14].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('all of em')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[25:33].to_list() + slope_data.columns[22:25].to_list() + slope_data.columns[17:20].to_list() + demo_columns\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p, num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    return group_c, group_p, group_titles\n",
    "\n",
    "\n",
    "def combos_with_demo(slope_data, demo_col):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_titles = []\n",
    "    \n",
    "    #demo_columns = [slope_data.columns[36], slope_data.columns[38], slope_data.columns[41], slope_data.columns[43]]\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    columns = slope_data.columns[2:14].to_list() + [demo_col]\n",
    "    X0_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    columns = slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p, num_samples = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    for col in slope_data.columns[17:20].to_list():\n",
    "        group_titles.append(col)\n",
    "        X0 = slope_data[col].to_numpy(copy=True)\n",
    "        c,p,num_samples = get_predictions(X0, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "        group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p, num_samples = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    columns = slope_data.columns[22:25].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    for col in slope_data.columns[22:25].to_list():\n",
    "        group_titles.append(col)\n",
    "        X0 = slope_data[[col, demo_col]].to_numpy(copy=True)\n",
    "        c,p,num_samples = get_combo_predictions(X0, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "        group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('jitter, shimmer, pauses')\n",
    "    columns = slope_data.columns[22:25].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('mfccs + jitter, shimmer')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:25].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('mfccs + jitter shimmer + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:25].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    columns = slope_data.columns[25:33].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    for col in slope_data.columns[25:33].to_list():\n",
    "        group_titles.append(col)\n",
    "        X0 = slope_data[[col, demo_col]].to_numpy(copy=True)\n",
    "        c, p, num_samples = get_combo_predictions(X0, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "        group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('bio + jitter, shimmer')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[22:25].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('bio + jitter, shimmer + pauses')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[22:25].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('bio + pauses')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[25:33].to_list() + [demo_col]\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p,num_samples = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    group_titles.append('all of em')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[25:33].to_list() + slope_data.columns[22:25].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p, num_samples = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    group_num_samples.append(num_samples)\n",
    "    \n",
    "    return group_c, group_p, group_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def generate_graphs(c, p, num, titles, specifier):\n",
    "    short_y_col = [\"CAI St(19)\", \"CAI F(18)\" , \"STAI T(17)\"]\n",
    "    correlations = np.matrix(c)\n",
    "    pvalues = np.matrix(p)\n",
    "    sample_sizes = np.matrix(num)\n",
    "\n",
    "    correlations = np.round(correlations, decimals=2)\n",
    "    pvalues = np.round(pvalues, decimals=2)\n",
    "    \n",
    "    graph_title = \"correlations_\" + specifier + \".png\"\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,85))\n",
    "        \n",
    "    p = ax.pcolor(correlations, vmin=-0.5, vmax=0.8)\n",
    "    fig.colorbar(p, ax=ax, fraction=0.05, pad=0.04)\n",
    "    ax.set_xticklabels(labels=short_y_col)\n",
    "    ax.set_yticklabels(labels=titles)\n",
    "    plt.yticks(np.arange(0, len(titles), 1.0))\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(30)  \n",
    "\n",
    "    for i in range(len(short_y_col)):\n",
    "        for j in range(len(titles)):\n",
    "            txt = 'c=' + str(correlations[j,i]) + ', p=' + str(pvalues[j,i]) + ', n=' + sample_sizes[j, i])\n",
    "            text = ax.text(i + 0.5, j + 0.5, txt, ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(DATA_PATH + 'expanded_features/' + graph_title)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
