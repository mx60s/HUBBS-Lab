{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>trial</th>\n",
       "      <th>pcm_RMSenergy_sma_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[1]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[2]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[3]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[4]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[5]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[6]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[7]_amean</th>\n",
       "      <th>...</th>\n",
       "      <th>CAI Trait Full Score</th>\n",
       "      <th>STAI Trait Score</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Lang</th>\n",
       "      <th>college</th>\n",
       "      <th>presentation</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>presentation_3_months</th>\n",
       "      <th>highest_education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007879</td>\n",
       "      <td>0.956435</td>\n",
       "      <td>-13.176012</td>\n",
       "      <td>-4.782369</td>\n",
       "      <td>-5.508350</td>\n",
       "      <td>-6.742012</td>\n",
       "      <td>-13.622468</td>\n",
       "      <td>-4.198480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>-0.399977</td>\n",
       "      <td>-12.222916</td>\n",
       "      <td>-8.252062</td>\n",
       "      <td>-2.672821</td>\n",
       "      <td>-3.437455</td>\n",
       "      <td>-14.641911</td>\n",
       "      <td>-2.807243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.615522</td>\n",
       "      <td>-9.837350</td>\n",
       "      <td>-2.540836</td>\n",
       "      <td>-2.281998</td>\n",
       "      <td>-3.958653</td>\n",
       "      <td>-14.158829</td>\n",
       "      <td>1.465646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.912469</td>\n",
       "      <td>-8.391797</td>\n",
       "      <td>-7.296996</td>\n",
       "      <td>-3.236593</td>\n",
       "      <td>-1.216842</td>\n",
       "      <td>-14.971951</td>\n",
       "      <td>-1.150219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>-1.409059</td>\n",
       "      <td>-1.801911</td>\n",
       "      <td>-5.151633</td>\n",
       "      <td>-2.803752</td>\n",
       "      <td>-6.043437</td>\n",
       "      <td>-5.157262</td>\n",
       "      <td>-2.180702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>73</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>-5.346863</td>\n",
       "      <td>-4.401036</td>\n",
       "      <td>-1.772596</td>\n",
       "      <td>-6.538305</td>\n",
       "      <td>5.228556</td>\n",
       "      <td>-17.409770</td>\n",
       "      <td>-3.816441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>73</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.019861</td>\n",
       "      <td>-6.544580</td>\n",
       "      <td>-0.529959</td>\n",
       "      <td>-4.315085</td>\n",
       "      <td>-7.491222</td>\n",
       "      <td>9.523307</td>\n",
       "      <td>-22.557555</td>\n",
       "      <td>-2.752085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>73</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.023864</td>\n",
       "      <td>-6.224142</td>\n",
       "      <td>-1.605599</td>\n",
       "      <td>-4.598936</td>\n",
       "      <td>-8.447270</td>\n",
       "      <td>9.679458</td>\n",
       "      <td>-22.852172</td>\n",
       "      <td>-0.745983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>73</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.019280</td>\n",
       "      <td>-6.070041</td>\n",
       "      <td>-1.986066</td>\n",
       "      <td>-4.606451</td>\n",
       "      <td>-5.151283</td>\n",
       "      <td>6.730979</td>\n",
       "      <td>-23.007388</td>\n",
       "      <td>-1.617074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>73</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.018839</td>\n",
       "      <td>-5.079182</td>\n",
       "      <td>-1.334333</td>\n",
       "      <td>-3.884128</td>\n",
       "      <td>-7.937570</td>\n",
       "      <td>9.279237</td>\n",
       "      <td>-19.915600</td>\n",
       "      <td>-4.012684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  trial  pcm_RMSenergy_sma_amean  pcm_fftMag_mfcc_sma[1]_amean  \\\n",
       "0     4    1.0                 0.007879                      0.956435   \n",
       "1     4    2.0                 0.006843                     -0.399977   \n",
       "2     4    3.0                 0.003532                      0.615522   \n",
       "3     4    4.0                 0.004056                      0.912469   \n",
       "4     4    5.0                 0.000788                     -1.409059   \n",
       "..   ..    ...                      ...                           ...   \n",
       "147  73    4.0                 0.016747                     -5.346863   \n",
       "148  73    5.0                 0.019861                     -6.544580   \n",
       "149  73    6.0                 0.023864                     -6.224142   \n",
       "150  73    7.0                 0.019280                     -6.070041   \n",
       "151  73    8.0                 0.018839                     -5.079182   \n",
       "\n",
       "     pcm_fftMag_mfcc_sma[2]_amean  pcm_fftMag_mfcc_sma[3]_amean  \\\n",
       "0                      -13.176012                     -4.782369   \n",
       "1                      -12.222916                     -8.252062   \n",
       "2                       -9.837350                     -2.540836   \n",
       "3                       -8.391797                     -7.296996   \n",
       "4                       -1.801911                     -5.151633   \n",
       "..                            ...                           ...   \n",
       "147                     -4.401036                     -1.772596   \n",
       "148                     -0.529959                     -4.315085   \n",
       "149                     -1.605599                     -4.598936   \n",
       "150                     -1.986066                     -4.606451   \n",
       "151                     -1.334333                     -3.884128   \n",
       "\n",
       "     pcm_fftMag_mfcc_sma[4]_amean  pcm_fftMag_mfcc_sma[5]_amean  \\\n",
       "0                       -5.508350                     -6.742012   \n",
       "1                       -2.672821                     -3.437455   \n",
       "2                       -2.281998                     -3.958653   \n",
       "3                       -3.236593                     -1.216842   \n",
       "4                       -2.803752                     -6.043437   \n",
       "..                            ...                           ...   \n",
       "147                     -6.538305                      5.228556   \n",
       "148                     -7.491222                      9.523307   \n",
       "149                     -8.447270                      9.679458   \n",
       "150                     -5.151283                      6.730979   \n",
       "151                     -7.937570                      9.279237   \n",
       "\n",
       "     pcm_fftMag_mfcc_sma[6]_amean  pcm_fftMag_mfcc_sma[7]_amean  ...  \\\n",
       "0                      -13.622468                     -4.198480  ...   \n",
       "1                      -14.641911                     -2.807243  ...   \n",
       "2                      -14.158829                      1.465646  ...   \n",
       "3                      -14.971951                     -1.150219  ...   \n",
       "4                       -5.157262                     -2.180702  ...   \n",
       "..                            ...                           ...  ...   \n",
       "147                    -17.409770                     -3.816441  ...   \n",
       "148                    -22.557555                     -2.752085  ...   \n",
       "149                    -22.852172                     -0.745983  ...   \n",
       "150                    -23.007388                     -1.617074  ...   \n",
       "151                    -19.915600                     -4.012684  ...   \n",
       "\n",
       "     CAI Trait Full Score  STAI Trait Score  Age  Gender  Lang  college  \\\n",
       "0                0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "1                0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "2                0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "3                0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "4                0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "..                    ...               ...  ...     ...   ...      ...   \n",
       "147              0.233333          0.033333  1.0     2.0   1.0      1.0   \n",
       "148              0.233333          0.033333  1.0     2.0   1.0      1.0   \n",
       "149              0.233333          0.033333  1.0     2.0   1.0      1.0   \n",
       "150              0.233333          0.033333  1.0     2.0   1.0      1.0   \n",
       "151              0.233333          0.033333  1.0     2.0   1.0      1.0   \n",
       "\n",
       "     presentation  ethnicity  presentation_3_months  highest_education  \n",
       "0             2.0        1.0                    0.0                3.0  \n",
       "1             2.0        1.0                    0.0                3.0  \n",
       "2             2.0        1.0                    0.0                3.0  \n",
       "3             2.0        1.0                    0.0                3.0  \n",
       "4             2.0        1.0                    0.0                3.0  \n",
       "..            ...        ...                    ...                ...  \n",
       "147           1.0        3.0                    2.0                1.0  \n",
       "148           1.0        3.0                    2.0                1.0  \n",
       "149           1.0        3.0                    2.0                1.0  \n",
       "150           1.0        3.0                    2.0                1.0  \n",
       "151           1.0        3.0                    2.0                1.0  \n",
       "\n",
       "[152 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "#############################\n",
    "# Import and combine all data\n",
    "#############################\n",
    "\n",
    "\n",
    "DATA_PATH = \"/Users/mvonebers/HUBBS-Lab/data/\"\n",
    "#DATA_PATH = \"/home/maggie/HUBBS-Lab/data/\"\n",
    "\n",
    "e4_data = pd.read_excel(DATA_PATH + \"E4_TEST.xlsx\")\n",
    "change_data = pd.read_excel(DATA_PATH + \"normalized_change.xlsx\")\n",
    "audio_data = pd.read_excel(DATA_PATH + \"audio_TEST.xlsx\")\n",
    "demo_data = pd.read_csv(DATA_PATH + \"Demographics Information.csv\")\n",
    "\n",
    "\n",
    "# Break apart the ID column into \"person\" and \"trial\"\n",
    "def clean_id(data):\n",
    "    data.insert(0, \"person\", [0] * data.shape[0])\n",
    "    data.insert(1, \"trial\", [0] * data.shape[0])\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        data.at[i, \"person\"] = int(data.at[i, \"id\"][7:])\n",
    "        data.at[i, \"trial\"] = int(data.at[i, \"id\"][5])\n",
    "    \n",
    "    data = data.drop(columns=['id'])\n",
    "    data = data.rename(columns={\"person\": \"id\"})\n",
    "    return data\n",
    "\n",
    "    \n",
    "e4_data = clean_id(e4_data)\n",
    "audio_data = clean_id(audio_data)    \n",
    "    \n",
    "all_data = pd.merge(e4_data, change_data, on='id')\n",
    "all_data = audio_data.merge(all_data, how='right')\n",
    "\n",
    "\n",
    "# Reorder survey data in order of most samples to least\n",
    "columns = all_data.columns.to_list()\n",
    "new_columns = deepcopy(columns)\n",
    "new_columns[35] = columns[40]\n",
    "new_columns[37] = columns[41] \n",
    "new_columns[38] = columns[37] \n",
    "new_columns[40] = columns[35]\n",
    "new_columns[41] = columns[38]\n",
    "\n",
    "all_data = all_data[new_columns]\n",
    "\n",
    "\n",
    "# what does this do?\n",
    "#demo_ids = demo_data['id'].to_list()\n",
    "#\n",
    "#for id_ in demo_ids:\n",
    "#    if id_ not in slope_ids:\n",
    "#        demo_data = demo_data[demo_data.id != id_]\n",
    "        \n",
    "all_data = all_data.merge(demo_data, how=\"right\")\n",
    "\n",
    "# The demographic data lists IDs that aren't present in the other data, so remove them\n",
    "all_data = all_data[all_data.id != 16]\n",
    "all_data = all_data[all_data.id != 27]\n",
    "all_data = all_data[all_data.id != 38]\n",
    "all_data = all_data[all_data.id != 43]\n",
    "all_data = all_data[all_data.id != 46]\n",
    "all_data = all_data[all_data.id != 49]\n",
    "all_data = all_data[all_data.id != 53]\n",
    "all_data = all_data[all_data.id != 58]\n",
    "all_data = all_data[all_data.id != 65]\n",
    "all_data = all_data[all_data.id != 66]\n",
    "\n",
    "all_data.drop(['CAI Trait Small group Score', 'CAI Trait Dyadic Score', 'CAI Trait Public Speaking Score', 'STAI State Score', 'Brief fear of Negative Evaluation'], axis=1, inplace=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 id\n",
      "1 pcm_RMSenergy_sma_amean\n",
      "2 pcm_fftMag_mfcc_sma[1]_amean\n",
      "3 pcm_fftMag_mfcc_sma[2]_amean\n",
      "4 pcm_fftMag_mfcc_sma[3]_amean\n",
      "5 pcm_fftMag_mfcc_sma[4]_amean\n",
      "6 pcm_fftMag_mfcc_sma[5]_amean\n",
      "7 pcm_fftMag_mfcc_sma[6]_amean\n",
      "8 pcm_fftMag_mfcc_sma[7]_amean\n",
      "9 pcm_fftMag_mfcc_sma[8]_amean\n",
      "10 pcm_fftMag_mfcc_sma[9]_amean\n",
      "11 pcm_fftMag_mfcc_sma[10]_amean\n",
      "12 pcm_fftMag_mfcc_sma[11]_amean\n",
      "13 pcm_fftMag_mfcc_sma[12]_amean\n",
      "14 pcm_zcr_sma_amean\n",
      "15 voiceProb_sma_amean\n",
      "16 F0_sma_amean\n",
      "17 #pause\n",
      "18 pause_frequency\n",
      "19 pause_interval\n",
      "20 mean\n",
      "21 percent\n",
      "22 jitterLocal_sma_amean\n",
      "23 jitterDDP_sma_amean\n",
      "24 shimmerLocal_sma_amean\n",
      "25 EDA_PPT\n",
      "26 HR_PPT\n",
      "27 TEMP_PPT\n",
      "28 BVP_PPT\n",
      "29 ACC_PPT\n",
      "30 IBI_PPT\n",
      "31 EDA_FREQ_PPT\n",
      "32 EDA_AMP_PPT\n",
      "33 Brief fear of Negative Evaluation\n",
      "34 CAI State Score\n",
      "35 CAI Trait Full Score\n",
      "36 STAI Trait Score\n",
      "37 Age\n",
      "38 Gender\n",
      "39 Lang\n",
      "40 college\n",
      "41 presentation\n",
      "42 ethnicity\n",
      "43 presentation_3_months\n",
      "44 highest_education\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Get slopes from linear regression of the 8 trials for each ID\n",
    "###############################################################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def get_slopes(data, start, end):\n",
    "    y0 = data['trial'].to_numpy(copy=True)\n",
    "\n",
    "    slopes = pd.DataFrame(np.zeros((19, 46)), columns=data.columns)\n",
    "    slopes = slopes.drop([\"trial\"], axis=1)\n",
    "\n",
    "    for col in range(2, 34):\n",
    "        x1 = data[data.columns[col]]\n",
    "        y0 = list(range(start, end + 1))\n",
    "        for row in range(19):\n",
    "            x0 = x1[ (row * 8) + start - 1 : (row * 8) + end ].to_numpy()\n",
    "            x = np.array([])\n",
    "            y = np.array([])\n",
    "            \n",
    "            slopes.iloc[row, 0] = data.iloc[row * 8, 0]\n",
    "\n",
    "            for i in range(len(x0)):  # remove NaN from data\n",
    "                if not math.isnan(x0[i]) and not math.isnan(y0[i]):\n",
    "                    x = np.append(x, x0[i])\n",
    "                    y = np.append(y, y0[i])\n",
    "            \n",
    "            try:\n",
    "                reg = LinearRegression().fit(y.reshape(-1,1),x)\n",
    "                slopes.iloc[row, col - 1] = reg.coef_\n",
    "            except:\n",
    "                 slopes.iloc[row, col - 1] = 0\n",
    "    \n",
    "    for col in range(34, 46):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] = data.iloc[row * 8, col]\n",
    "            \n",
    "    # Want to preserve zeros in the demographic data, so temporarily boost it up one...\n",
    "    for col in range(43, 46):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] += 1.0\n",
    "                    \n",
    "    slopes.replace(0, np.NaN, inplace=True)\n",
    "    \n",
    "    # Then bump it back down. (I know this is a dumb way to do this.)\n",
    "    for col in range(43, 46):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] -= 1.0\n",
    "    \n",
    "    return slopes\n",
    "\n",
    "all_slopes = get_slopes(all_data, 1, 8)\n",
    "slope_ids = all_slopes['id'].to_list()\n",
    "#slope_ids\n",
    "all_slopes\n",
    "\n",
    "for i, col in zip(range(len(all_slopes.columns.to_list())), all_slopes.columns.to_list()):\n",
    "    print(i, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "y_columns = change_data.columns.to_list()[1:]\n",
    "\n",
    "def get_combo_predictions(X0, slope_data):\n",
    "    corrs = []\n",
    "    ps = []\n",
    "    for y_col, y_i in zip(y_columns, range(len(y_columns))):\n",
    "        y0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        X = np.array([X0[0]])\n",
    "        y = np.array(y0[0])\n",
    "        \n",
    "        for i in range(1,len(X0)):  # remove NaN from data\n",
    "            is_nan = False\n",
    "            for x in X0[i]:\n",
    "                if math.isnan(x):\n",
    "                    is_nan = True\n",
    "                    break\n",
    "            if not math.isnan(y0[i]) and not is_nan:\n",
    "                X = np.append(X, [X0[i]], axis=0)\n",
    "                y = np.append(y, y0[i])\n",
    "            \n",
    "        folds = min(10, len(X))\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(folds, shuffle=True, random_state=42)\n",
    "        predicted_vals0 = cross_val_predict(model, X, y, cv=cv)\n",
    "        actual_vals0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        predicted_vals = []\n",
    "        actual_vals = []\n",
    "        \n",
    "\n",
    "        for j in range(len(predicted_vals0)):\n",
    "            if not math.isnan(predicted_vals0[j]) and not math.isnan(actual_vals0[j]):\n",
    "                predicted_vals.append(predicted_vals0[j])\n",
    "                actual_vals.append(actual_vals0[j])\n",
    "\n",
    "        correlation, pval = pearsonr(predicted_vals, actual_vals)\n",
    "        #to_print = str(correlation) + str(pval)\n",
    "        #if pval < 0.15: \n",
    "        #    if not folds == 10:\n",
    "        #        print(\"With # KFolds\", folds)\n",
    "        #    print(\"{0}:\\t\\t{1}\\t\\t{2}\\t{3}\\t\\t{4}\".format(y_col, correlation, pval, \"Rows:\", len(y)))\n",
    "        corrs.append(correlation)\n",
    "        ps.append(pval)\n",
    "        \n",
    "    return corrs, ps\n",
    "      \n",
    "def generate_combos(slope_data):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_titles = []\n",
    "    \n",
    "    group_titles.append('all demos')\n",
    "    X0_demo = slope_data[slope_data.columns[37:44]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_demo, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    \n",
    "    group_titles.append(\"bios + demos\")\n",
    "    columns = slope_data.columns[23:30].to_list() + slope_data.columns[37:44].to_list()\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('mfcc1-12 + demos')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[37:44].to_list()\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    X0_mfcc = slope_data[slope_data.columns[2:14]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    X0_pauses = slope_data[slope_data.columns[17:20]].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    X0_jitter = slope_data[slope_data.columns[22:24]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('jitter, shimmer, pauses')\n",
    "    columns = slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + jitter, shimmer')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('mfccs + jitter shimmer + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    X0_eda = slope_data[slope_data.columns[23:30]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('all of em')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + slope_data.columns[37:44].to_list()\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    \n",
    "    #return group_c, group_p, group_titles\n",
    "\n",
    "    # All mfcc's combined with other attributes\n",
    "    \n",
    "        \n",
    "    return group_c, group_p, group_titles\n",
    "\n",
    "#group_c, group_p, group_titles = generate_combos(all_slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "short_y_col = [\"BFNE (19)\", \"CAI Dy (16)\", \"CAI F(18)\", \"CAI PS(17)\", \"CAI Sm(15)\", \"STAI T(17)\", \"CAI St(19)\", \"STAI St(18)\"]\n",
    "correlations = np.matrix(group_c)\n",
    "pvalues = np.matrix(group_p)\n",
    "\n",
    "correlations = np.round(correlations, decimals=2)\n",
    "pvalues = np.round(pvalues, decimals=2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,85))\n",
    "\n",
    "p = ax.matshow(correlations)\n",
    "fig.colorbar(p, ax=ax, fraction=0.05, pad=0.04)\n",
    "ax.set_xticklabels(labels=[''] + short_y_col)\n",
    "ax.set_yticklabels(labels=group_titles)\n",
    "plt.yticks(np.arange(0, len(group_titles), 1.0))\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(30)  \n",
    "    \n",
    "for i in range(len(short_y_col)):\n",
    "    for j in range(len(group_titles)):\n",
    "        text = ax.text(i, j, correlations[j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "plt.title('LinReg Correlations')\n",
    "fig.tight_layout()\n",
    "plt.savefig(DATA_PATH + 'correlatons.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax2 = plt.subplots(nrows=1, ncols=1, figsize=(20,85))\n",
    "p = ax2.matshow(pvalues)\n",
    "fig.colorbar(p, ax=ax2, fraction=0.05, pad=0.04)\n",
    "ax2.set_xticklabels(labels=[''] + short_y_col)  \n",
    "ax2.set_yticklabels(labels=group_titles)\n",
    "plt.yticks(np.arange(0, len(group_titles), 1.0))\n",
    "for tick in ax2.get_xticklabels():\n",
    "    tick.set_rotation(30)\n",
    "    \n",
    "for i in range(len(short_y_col)):\n",
    "    for j in range(len(group_titles)):\n",
    "        text = ax2.text(i, j, pvalues[j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.title('LinReg Pvalues')\n",
    "fig.tight_layout()\n",
    "plt.savefig(DATA_PATH + 'pvalues.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#slopes_3 = get_slopes(all_data, 1, 3)\\n#c3, p3, titles3 = generate_combos(slopes_3)\\n#generate_graphs(c3, p3, titles3, '3')\\n\\n#slopes_3 = get_slopes(all_data, 5)\\n#c3, p3, titles3 = generate_combos(slopes_3)\\n#generate_graphs(c3, p3, titles3, '5')\\n\\nslopes_all = get_slopes(all_data, 1, 8)\\nc, p, titles = generate_combos(slopes_all)\\ngenerate_graphs(c, p, titles, 'all')\\n\\nfirst_4 = get_slopes(all_data, 1, 4)\\nc4, p4, titles4 = generate_combos(first_4)\\ngenerate_graphs(c4, p4, titles4, 'first4')\\n\\nmiddle_4 = get_slopes(all_data, 3, 6)\\nc4, p4, titles4 = generate_combos(middle_4)\\ngenerate_graphs(c4, p4, titles4, 'middle4')\\n\\nlast_4 = get_slopes(all_data, 5, 8)\\nc4, p4, titles4 = generate_combos(last_4)\\ngenerate_graphs(c4, p4, titles4, 'last4')\\n\\nfor i in range(1, 5):\\n    title = str(2*i - 1) + '-' + str(2*i)\\n    pair = get_slopes(all_data, 2*i - 1, 2*i)\\n    c2, p2, titles2 = generate_combos(pair)\\n    generate_graphs(c2, p2, titles2, title)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def generate_graphs(c, p, titles, specifier):\n",
    "    short_y_col = [\"BFNE (19)\", \"CAI St(19)\", \"CAI F(18)\" , \"STAI St(18)\", \"CAI PS(17)\", \"STAI T(17)\", \"CAI Dy (16)\", \"CAI Sm(15)\"]\n",
    "    correlations = np.matrix(c)\n",
    "    pvalues = np.matrix(p)\n",
    "\n",
    "    correlations = np.round(correlations, decimals=2)\n",
    "    pvalues = np.round(pvalues, decimals=2)\n",
    "    \n",
    "    data = [correlations, pvalues]\n",
    "    graph_titles = [\"correlations_\" + specifier + \".png\", \"pvalues_\" + specifier + \".png\"]\n",
    "    \n",
    "    for k in range(2):\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,85))\n",
    "        \n",
    "        if k == 0:\n",
    "            p = ax.pcolor(data[k], vmin=-0.5, vmax=0.8)\n",
    "        else:\n",
    "            p = ax.pcolor(data[k], vmin = 0.0, vmax = 1.0)\n",
    "        #p = ax.matshow(data[k])\n",
    "        fig.colorbar(p, ax=ax, fraction=0.05, pad=0.04)\n",
    "        ax.set_xticklabels(labels=short_y_col)\n",
    "        ax.set_yticklabels(labels=titles)\n",
    "        plt.yticks(np.arange(0, len(titles), 1.0))\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(30)  \n",
    "\n",
    "        for i in range(len(short_y_col)):\n",
    "            for j in range(len(titles)):\n",
    "                text = ax.text(i + 0.5, j + 0.5, data[k][j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(DATA_PATH + graph_titles[k])\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "\"\"\"#slopes_3 = get_slopes(all_data, 1, 3)\n",
    "#c3, p3, titles3 = generate_combos(slopes_3)\n",
    "#generate_graphs(c3, p3, titles3, '3')\n",
    "\n",
    "#slopes_3 = get_slopes(all_data, 5)\n",
    "#c3, p3, titles3 = generate_combos(slopes_3)\n",
    "#generate_graphs(c3, p3, titles3, '5')\n",
    "\n",
    "slopes_all = get_slopes(all_data, 1, 8)\n",
    "c, p, titles = generate_combos(slopes_all)\n",
    "generate_graphs(c, p, titles, 'all')\n",
    "\n",
    "first_4 = get_slopes(all_data, 1, 4)\n",
    "c4, p4, titles4 = generate_combos(first_4)\n",
    "generate_graphs(c4, p4, titles4, 'first4')\n",
    "\n",
    "middle_4 = get_slopes(all_data, 3, 6)\n",
    "c4, p4, titles4 = generate_combos(middle_4)\n",
    "generate_graphs(c4, p4, titles4, 'middle4')\n",
    "\n",
    "last_4 = get_slopes(all_data, 5, 8)\n",
    "c4, p4, titles4 = generate_combos(last_4)\n",
    "generate_graphs(c4, p4, titles4, 'last4')\n",
    "\n",
    "for i in range(1, 5):\n",
    "    title = str(2*i - 1) + '-' + str(2*i)\n",
    "    pair = get_slopes(all_data, 2*i - 1, 2*i)\n",
    "    c2, p2, titles2 = generate_combos(pair)\n",
    "    generate_graphs(c2, p2, titles2, title)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished age1\n",
      "Finished gender1\n",
      "Finished gender2\n",
      "Finished college1\n",
      "Finished college3\n",
      "Finished ethnicity1\n",
      "Finished ethnicity2\n",
      "Finished presentation1\n",
      "Finished edu1\n"
     ]
    }
   ],
   "source": [
    "def analyze_by_demo(slopes):\n",
    "    slopes.dropna(inplace=True)\n",
    "    age1 = slopes[slopes.Age == 1.0]\n",
    "    age2 = slopes[slopes.Age == 2.0]\n",
    "    age3 = slopes[slopes.Age == 3.0]\n",
    "    gender1 = slopes[slopes.Gender == 1.0]\n",
    "    gender2 = slopes[slopes.Gender == 2.0]\n",
    "    college1 = slopes[slopes.college == 1.0]\n",
    "    college2 = slopes[slopes.college == 2.0]\n",
    "    college3 = slopes[slopes.college == 3.0]\n",
    "    college4 = slopes[slopes.college == 4.0]\n",
    "    ethnicity1 = slopes[slopes.ethnicity == 1.0]\n",
    "    ethnicity2 = slopes[slopes.ethnicity == 2.0]\n",
    "    ethnicity3 = slopes[slopes.ethnicity == 3.0]\n",
    "    ethnicity4 = slopes[slopes.ethnicity == 4.0]\n",
    "    presentation0 = slopes[slopes.presentation_3_months == 0.0]\n",
    "    presentation1 = slopes[slopes.presentation_3_months == 1.0]\n",
    "    presentation2 = slopes[slopes.presentation_3_months == 2.0]\n",
    "    edu1 = slopes[slopes.highest_education == 1.0]\n",
    "    edu2 = slopes[slopes.highest_education == 2.0]\n",
    "    edu3 = slopes[slopes.highest_education == 2.0]\n",
    "    \n",
    "    all_demos = [age1, age2, age3, gender1, gender2, college1, \n",
    "                 college2, college3, college4, ethnicity1, \n",
    "                 ethnicity2, ethnicity3, ethnicity4, presentation0,\n",
    "                presentation1, presentation2, edu1, edu2, edu3]\n",
    "    demo_titles = [\"age1\", \"age2\", \"age3\", \"gender1\", \"gender2\", \"college1\", \"college2\", \n",
    "                   \"college3\", \"college4\", \"ethnicity1\", \"ethnicity2\", \"ethnicity3\", \n",
    "                   \"ethnicity4\", \"presentation0\", \"presentation1\", \"presentation2\", \"edu1\", \n",
    "                   \"edu2\", \"edu3\"]\n",
    "    \n",
    "    for d, t in zip(all_demos, demo_titles):\n",
    "        if d.shape[0] < 3:\n",
    "            continue\n",
    "        print(t, \"had\", d.shape[0], \"samples\")\n",
    "        c, p, titles = generate_combos(d)\n",
    "        print(\"Finished\", t)\n",
    "        generate_graphs(c, p, titles, t)\n",
    "        \n",
    "analyze_by_demo(all_slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combos_with_demo(slope_data, demo_col):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_titles = []\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    columns = slope_data.columns[2:14].to_list() + [demo_col]\n",
    "    X0_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    columns = slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    columns = slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs+ jitter, shimmer')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    columns = slope_data.columns[23:30].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list() + [demo_col]\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    #return group_c, group_p, group_titles\n",
    "\n",
    "    # All mfcc's combined with other attributes\n",
    "    for i in range(14, 25):\n",
    "        title = \"mfccs + \" + str(slope_data.columns[i])\n",
    "        group_titles.append(title)\n",
    "        columns = slope_data.columns[2:14].to_list() + [slope_data.columns[i]] + [demo_col]\n",
    "        X0 = slope_data[columns].to_numpy(copy=True)\n",
    "        c,p = get_combo_predictions(X0, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "\n",
    "    for i in range(14, 25):\n",
    "        col1 = slope_data.columns[i]\n",
    "        for j in range(i + 1, 25):\n",
    "            col2 = slope_data.columns[j]\n",
    "            title = str(col1) + '+' + str(col2)\n",
    "            group_titles.append(title)\n",
    "            X0 = slope_data[[col1, col2, demo_col]].to_numpy(copy=True)\n",
    "            c, p = get_combo_predictions(X0, slope_data)\n",
    "            group_c.append(c)\n",
    "            group_p.append(p)\n",
    "\n",
    "    for i in range(14, 25):\n",
    "        col1 = slope_data.columns[i]\n",
    "        for j in range(i + 1, 25):\n",
    "            col2 = slope_data.columns[j]\n",
    "            columns = slope_data.columns[2:14].to_list() + [col1] + [col2] + [demo_col]\n",
    "            title = \"mfccs + \" + str(col1) + '+' + str(col2)\n",
    "            group_titles.append(title)\n",
    "            X0 = slope_data[[col1, col2]].to_numpy(copy=True)\n",
    "            c, p = get_combo_predictions(X0, slope_data)\n",
    "            group_c.append(c)\n",
    "            group_p.append(p)\n",
    "\n",
    "    for i in range(14, 24):\n",
    "        col = slope_data.columns[i]\n",
    "        title = 'bio +' + str(col)\n",
    "        group_titles.append(title)\n",
    "        columns = slope_data.columns[23:30].to_list() + [col] + [demo_col]\n",
    "        X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "        c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "\n",
    "    for i in range(14, 24):\n",
    "        col = slope_data.columns[i]\n",
    "        title = 'mfcc + bio +' + str(col)\n",
    "        group_titles.append(title)\n",
    "        columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list() + [col] + [demo_col]\n",
    "        X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "        c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "        \n",
    "    return group_c, group_p, group_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo_title in demo_data.columns.to_list():\n",
    "    #print(demo_title)\n",
    "    c, p, titles = generate_combos_with_demo(all_slopes, demo_title)\n",
    "    generate_graphs(c, p, titles, demo_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = all_slopes.columns[1:33].to_list()\n",
    "X0 = all_slopes[columns].to_numpy(copy=True)\n",
    "c, p = get_combo_predictions(X0, all_slopes)\n",
    "generate_graphs([c], [p], [\"all\"], \"all-no-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
