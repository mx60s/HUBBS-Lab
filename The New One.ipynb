{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>trial</th>\n",
       "      <th>pcm_RMSenergy_sma_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[1]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[2]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[3]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[4]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[5]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[6]_amean</th>\n",
       "      <th>pcm_fftMag_mfcc_sma[7]_amean</th>\n",
       "      <th>...</th>\n",
       "      <th>CAI Trait Full Score</th>\n",
       "      <th>STAI Trait Score</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Lang</th>\n",
       "      <th>college</th>\n",
       "      <th>presentation</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>presentation_3_months</th>\n",
       "      <th>highest_education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007879</td>\n",
       "      <td>0.956435</td>\n",
       "      <td>-13.176012</td>\n",
       "      <td>-4.782369</td>\n",
       "      <td>-5.508350</td>\n",
       "      <td>-6.742012</td>\n",
       "      <td>-13.622468</td>\n",
       "      <td>-4.198480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>-0.399977</td>\n",
       "      <td>-12.222916</td>\n",
       "      <td>-8.252062</td>\n",
       "      <td>-2.672821</td>\n",
       "      <td>-3.437455</td>\n",
       "      <td>-14.641911</td>\n",
       "      <td>-2.807243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.615522</td>\n",
       "      <td>-9.837350</td>\n",
       "      <td>-2.540836</td>\n",
       "      <td>-2.281998</td>\n",
       "      <td>-3.958653</td>\n",
       "      <td>-14.158829</td>\n",
       "      <td>1.465646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.912469</td>\n",
       "      <td>-8.391797</td>\n",
       "      <td>-7.296996</td>\n",
       "      <td>-3.236593</td>\n",
       "      <td>-1.216842</td>\n",
       "      <td>-14.971951</td>\n",
       "      <td>-1.150219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>-1.409059</td>\n",
       "      <td>-1.801911</td>\n",
       "      <td>-5.151633</td>\n",
       "      <td>-2.803752</td>\n",
       "      <td>-6.043437</td>\n",
       "      <td>-5.157262</td>\n",
       "      <td>-2.180702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>-5.122845</td>\n",
       "      <td>-2.803909</td>\n",
       "      <td>-10.329738</td>\n",
       "      <td>-3.283098</td>\n",
       "      <td>-8.667224</td>\n",
       "      <td>-5.319049</td>\n",
       "      <td>-5.739635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>-6.323907</td>\n",
       "      <td>-4.457356</td>\n",
       "      <td>-10.233436</td>\n",
       "      <td>-7.749134</td>\n",
       "      <td>-6.905997</td>\n",
       "      <td>-1.712893</td>\n",
       "      <td>-9.354967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>-6.278327</td>\n",
       "      <td>-5.177833</td>\n",
       "      <td>-8.066029</td>\n",
       "      <td>-9.880027</td>\n",
       "      <td>-8.082714</td>\n",
       "      <td>-3.221872</td>\n",
       "      <td>-9.710517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.068182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.028753</td>\n",
       "      <td>-4.441762</td>\n",
       "      <td>-4.290134</td>\n",
       "      <td>-6.740623</td>\n",
       "      <td>-17.795074</td>\n",
       "      <td>9.291694</td>\n",
       "      <td>-22.565406</td>\n",
       "      <td>-4.937093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>-3.683452</td>\n",
       "      <td>-2.650953</td>\n",
       "      <td>-6.539142</td>\n",
       "      <td>-18.121149</td>\n",
       "      <td>9.095162</td>\n",
       "      <td>-24.370033</td>\n",
       "      <td>-5.221596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.027262</td>\n",
       "      <td>-4.887059</td>\n",
       "      <td>-3.956519</td>\n",
       "      <td>-6.108426</td>\n",
       "      <td>-16.353674</td>\n",
       "      <td>8.965622</td>\n",
       "      <td>-24.726247</td>\n",
       "      <td>-2.926949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>-4.919045</td>\n",
       "      <td>-5.410709</td>\n",
       "      <td>-2.725021</td>\n",
       "      <td>-15.355330</td>\n",
       "      <td>6.358016</td>\n",
       "      <td>-22.504217</td>\n",
       "      <td>-2.552993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>-5.416044</td>\n",
       "      <td>-2.115671</td>\n",
       "      <td>-4.667829</td>\n",
       "      <td>-9.217310</td>\n",
       "      <td>-2.923792</td>\n",
       "      <td>-6.906242</td>\n",
       "      <td>-7.276462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>-4.778107</td>\n",
       "      <td>-3.785143</td>\n",
       "      <td>-6.385363</td>\n",
       "      <td>-7.197669</td>\n",
       "      <td>-3.711713</td>\n",
       "      <td>-8.736096</td>\n",
       "      <td>-5.613836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>-3.355013</td>\n",
       "      <td>-3.536216</td>\n",
       "      <td>-3.345431</td>\n",
       "      <td>-8.250748</td>\n",
       "      <td>-3.087355</td>\n",
       "      <td>-8.452376</td>\n",
       "      <td>-6.206154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  trial  pcm_RMSenergy_sma_amean  pcm_fftMag_mfcc_sma[1]_amean  \\\n",
       "0    4    1.0                 0.007879                      0.956435   \n",
       "1    4    2.0                 0.006843                     -0.399977   \n",
       "2    4    3.0                 0.003532                      0.615522   \n",
       "3    4    4.0                 0.004056                      0.912469   \n",
       "4    4    5.0                 0.000788                     -1.409059   \n",
       "5    4    6.0                 0.001677                     -5.122845   \n",
       "6    4    7.0                 0.001685                     -6.323907   \n",
       "7    4    8.0                 0.002461                     -6.278327   \n",
       "8    5    1.0                 0.028753                     -4.441762   \n",
       "9    5    2.0                 0.029290                     -3.683452   \n",
       "10   5    3.0                 0.027262                     -4.887059   \n",
       "11   5    4.0                 0.021383                     -4.919045   \n",
       "12   5    5.0                 0.004166                     -5.416044   \n",
       "13   5    6.0                 0.004713                     -4.778107   \n",
       "14   5    7.0                 0.003858                     -3.355013   \n",
       "\n",
       "    pcm_fftMag_mfcc_sma[2]_amean  pcm_fftMag_mfcc_sma[3]_amean  \\\n",
       "0                     -13.176012                     -4.782369   \n",
       "1                     -12.222916                     -8.252062   \n",
       "2                      -9.837350                     -2.540836   \n",
       "3                      -8.391797                     -7.296996   \n",
       "4                      -1.801911                     -5.151633   \n",
       "5                      -2.803909                    -10.329738   \n",
       "6                      -4.457356                    -10.233436   \n",
       "7                      -5.177833                     -8.066029   \n",
       "8                      -4.290134                     -6.740623   \n",
       "9                      -2.650953                     -6.539142   \n",
       "10                     -3.956519                     -6.108426   \n",
       "11                     -5.410709                     -2.725021   \n",
       "12                     -2.115671                     -4.667829   \n",
       "13                     -3.785143                     -6.385363   \n",
       "14                     -3.536216                     -3.345431   \n",
       "\n",
       "    pcm_fftMag_mfcc_sma[4]_amean  pcm_fftMag_mfcc_sma[5]_amean  \\\n",
       "0                      -5.508350                     -6.742012   \n",
       "1                      -2.672821                     -3.437455   \n",
       "2                      -2.281998                     -3.958653   \n",
       "3                      -3.236593                     -1.216842   \n",
       "4                      -2.803752                     -6.043437   \n",
       "5                      -3.283098                     -8.667224   \n",
       "6                      -7.749134                     -6.905997   \n",
       "7                      -9.880027                     -8.082714   \n",
       "8                     -17.795074                      9.291694   \n",
       "9                     -18.121149                      9.095162   \n",
       "10                    -16.353674                      8.965622   \n",
       "11                    -15.355330                      6.358016   \n",
       "12                     -9.217310                     -2.923792   \n",
       "13                     -7.197669                     -3.711713   \n",
       "14                     -8.250748                     -3.087355   \n",
       "\n",
       "    pcm_fftMag_mfcc_sma[6]_amean  pcm_fftMag_mfcc_sma[7]_amean  ...  \\\n",
       "0                     -13.622468                     -4.198480  ...   \n",
       "1                     -14.641911                     -2.807243  ...   \n",
       "2                     -14.158829                      1.465646  ...   \n",
       "3                     -14.971951                     -1.150219  ...   \n",
       "4                      -5.157262                     -2.180702  ...   \n",
       "5                      -5.319049                     -5.739635  ...   \n",
       "6                      -1.712893                     -9.354967  ...   \n",
       "7                      -3.221872                     -9.710517  ...   \n",
       "8                     -22.565406                     -4.937093  ...   \n",
       "9                     -24.370033                     -5.221596  ...   \n",
       "10                    -24.726247                     -2.926949  ...   \n",
       "11                    -22.504217                     -2.552993  ...   \n",
       "12                     -6.906242                     -7.276462  ...   \n",
       "13                     -8.736096                     -5.613836  ...   \n",
       "14                     -8.452376                     -6.206154  ...   \n",
       "\n",
       "    CAI Trait Full Score  STAI Trait Score  Age  Gender  Lang  college  \\\n",
       "0               0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "1               0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "2               0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "3               0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "4               0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "5               0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "6               0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "7               0.101695         -0.068182  3.0     1.0   2.0      1.0   \n",
       "8               0.121212          0.025641  2.0     2.0   2.0      1.0   \n",
       "9               0.121212          0.025641  2.0     2.0   2.0      1.0   \n",
       "10              0.121212          0.025641  2.0     2.0   2.0      1.0   \n",
       "11              0.121212          0.025641  2.0     2.0   2.0      1.0   \n",
       "12              0.121212          0.025641  2.0     2.0   2.0      1.0   \n",
       "13              0.121212          0.025641  2.0     2.0   2.0      1.0   \n",
       "14              0.121212          0.025641  2.0     2.0   2.0      1.0   \n",
       "\n",
       "    presentation  ethnicity  presentation_3_months  highest_education  \n",
       "0            2.0        1.0                    0.0                3.0  \n",
       "1            2.0        1.0                    0.0                3.0  \n",
       "2            2.0        1.0                    0.0                3.0  \n",
       "3            2.0        1.0                    0.0                3.0  \n",
       "4            2.0        1.0                    0.0                3.0  \n",
       "5            2.0        1.0                    0.0                3.0  \n",
       "6            2.0        1.0                    0.0                3.0  \n",
       "7            2.0        1.0                    0.0                3.0  \n",
       "8            1.0        1.0                    1.0                2.0  \n",
       "9            1.0        1.0                    1.0                2.0  \n",
       "10           1.0        1.0                    1.0                2.0  \n",
       "11           1.0        1.0                    1.0                2.0  \n",
       "12           1.0        1.0                    1.0                2.0  \n",
       "13           1.0        1.0                    1.0                2.0  \n",
       "14           1.0        1.0                    1.0                2.0  \n",
       "\n",
       "[15 rows x 45 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "#############################\n",
    "# Import and combine all data\n",
    "#############################\n",
    "\n",
    "\n",
    "DATA_PATH = \"/Users/mvonebers/HUBBS-Lab/data/\"\n",
    "#DATA_PATH = \"/home/maggie/HUBBS-Lab/data/\"\n",
    "\n",
    "e4_data = pd.read_excel(DATA_PATH + \"E4_TEST.xlsx\")\n",
    "change_data = pd.read_excel(DATA_PATH + \"normalized_change.xlsx\")\n",
    "audio_data = pd.read_excel(DATA_PATH + \"audio_TEST.xlsx\")\n",
    "demo_data = pd.read_csv(DATA_PATH + \"Demographics Information.csv\")\n",
    "\n",
    "\n",
    "# Break apart the ID column into \"person\" and \"trial\"\n",
    "def clean_id(data):\n",
    "    data.insert(0, \"person\", [0] * data.shape[0])\n",
    "    data.insert(1, \"trial\", [0] * data.shape[0])\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        data.at[i, \"person\"] = int(data.at[i, \"id\"][7:])\n",
    "        data.at[i, \"trial\"] = int(data.at[i, \"id\"][5])\n",
    "    \n",
    "    data = data.drop(columns=['id'])\n",
    "    data = data.rename(columns={\"person\": \"id\"})\n",
    "    return data\n",
    "\n",
    "    \n",
    "e4_data = clean_id(e4_data)\n",
    "audio_data = clean_id(audio_data)    \n",
    "    \n",
    "all_data = pd.merge(e4_data, change_data, on='id')\n",
    "all_data = audio_data.merge(all_data, how='right')\n",
    "\n",
    "\n",
    "# Reorder survey data in order of most samples to least\n",
    "columns = all_data.columns.to_list()\n",
    "new_columns = deepcopy(columns)\n",
    "new_columns[35] = columns[40]\n",
    "new_columns[37] = columns[41] \n",
    "new_columns[38] = columns[37] \n",
    "new_columns[40] = columns[35]\n",
    "new_columns[41] = columns[38]\n",
    "\n",
    "all_data = all_data[new_columns]\n",
    "\n",
    "\n",
    "# what does this do?\n",
    "#demo_ids = demo_data['id'].to_list()\n",
    "#\n",
    "#for id_ in demo_ids:\n",
    "#    if id_ not in slope_ids:\n",
    "#        demo_data = demo_data[demo_data.id != id_]\n",
    "        \n",
    "all_data = all_data.merge(demo_data, how=\"right\")\n",
    "\n",
    "# The demographic data lists IDs that aren't present in the other data, so remove them\n",
    "all_data = all_data[all_data.id != 16]\n",
    "all_data = all_data[all_data.id != 27]\n",
    "all_data = all_data[all_data.id != 38]\n",
    "all_data = all_data[all_data.id != 43]\n",
    "all_data = all_data[all_data.id != 46]\n",
    "all_data = all_data[all_data.id != 49]\n",
    "all_data = all_data[all_data.id != 53]\n",
    "all_data = all_data[all_data.id != 58]\n",
    "all_data = all_data[all_data.id != 65]\n",
    "all_data = all_data[all_data.id != 66]\n",
    "\n",
    "all_data.drop(['CAI Trait Small group Score', 'CAI Trait Dyadic Score', 'CAI Trait Public Speaking Score', 'STAI State Score', 'Brief fear of Negative Evaluation'], axis=1, inplace=True)\n",
    "all_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 id\n",
      "1 pcm_RMSenergy_sma_amean\n",
      "2 pcm_fftMag_mfcc_sma[1]_amean\n",
      "3 pcm_fftMag_mfcc_sma[2]_amean\n",
      "4 pcm_fftMag_mfcc_sma[3]_amean\n",
      "5 pcm_fftMag_mfcc_sma[4]_amean\n",
      "6 pcm_fftMag_mfcc_sma[5]_amean\n",
      "7 pcm_fftMag_mfcc_sma[6]_amean\n",
      "8 pcm_fftMag_mfcc_sma[7]_amean\n",
      "9 pcm_fftMag_mfcc_sma[8]_amean\n",
      "10 pcm_fftMag_mfcc_sma[9]_amean\n",
      "11 pcm_fftMag_mfcc_sma[10]_amean\n",
      "12 pcm_fftMag_mfcc_sma[11]_amean\n",
      "13 pcm_fftMag_mfcc_sma[12]_amean\n",
      "14 pcm_zcr_sma_amean\n",
      "15 voiceProb_sma_amean\n",
      "16 F0_sma_amean\n",
      "17 #pause\n",
      "18 pause_frequency\n",
      "19 pause_interval\n",
      "20 mean\n",
      "21 percent\n",
      "22 jitterLocal_sma_amean\n",
      "23 jitterDDP_sma_amean\n",
      "24 shimmerLocal_sma_amean\n",
      "25 EDA_PPT\n",
      "26 HR_PPT\n",
      "27 TEMP_PPT\n",
      "28 BVP_PPT\n",
      "29 ACC_PPT\n",
      "30 IBI_PPT\n",
      "31 EDA_FREQ_PPT\n",
      "32 EDA_AMP_PPT\n",
      "33 CAI State Score\n",
      "34 CAI Trait Full Score\n",
      "35 STAI Trait Score\n",
      "36 Age\n",
      "37 Gender\n",
      "38 Lang\n",
      "39 college\n",
      "40 presentation\n",
      "41 ethnicity\n",
      "42 presentation_3_months\n",
      "43 highest_education\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Get slopes from linear regression of the 8 trials for each ID\n",
    "###############################################################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def get_slopes(data, start, end):\n",
    "    y0 = data['trial'].to_numpy(copy=True)\n",
    "\n",
    "    slopes = pd.DataFrame(np.zeros((19, 45)), columns=data.columns)\n",
    "    slopes = slopes.drop([\"trial\"], axis=1)\n",
    "\n",
    "    for col in range(2, 34):\n",
    "        x1 = data[data.columns[col]]\n",
    "        y0 = list(range(start, end + 1))\n",
    "        for row in range(19):\n",
    "            x0 = x1[ (row * 8) + start - 1 : (row * 8) + end ].to_numpy()\n",
    "            x = np.array([])\n",
    "            y = np.array([])\n",
    "            \n",
    "            slopes.iloc[row, 0] = data.iloc[row * 8, 0]\n",
    "\n",
    "            for i in range(len(x0)):  # remove NaN from data\n",
    "                if not math.isnan(x0[i]) and not math.isnan(y0[i]):\n",
    "                    x = np.append(x, x0[i])\n",
    "                    y = np.append(y, y0[i])\n",
    "            \n",
    "            try:\n",
    "                reg = LinearRegression().fit(y.reshape(-1,1),x)\n",
    "                slopes.iloc[row, col - 1] = reg.coef_\n",
    "            except:\n",
    "                 slopes.iloc[row, col - 1] = 0\n",
    "    \n",
    "    for col in range(34, 45):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] = data.iloc[row * 8, col]\n",
    "            \n",
    "    # Want to preserve zeros in the demographic data, so temporarily boost it up one...\n",
    "    for col in range(43, 45):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] += 1.0\n",
    "                    \n",
    "    slopes.replace(0, np.NaN, inplace=True)\n",
    "    \n",
    "    # Then bump it back down. (I know this is a dumb way to do this.)\n",
    "    for col in range(43, 45):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] -= 1.0\n",
    "    \n",
    "    return slopes\n",
    "\n",
    "all_slopes = get_slopes(all_data, 1, 8)\n",
    "\n",
    "for i, col in zip(range(len(all_slopes.columns)), all_slopes.columns):\n",
    "    print(i, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Problem: CAI State values are showing up as the CAI Dyadic...\n",
    "\n",
    "y_columns = ['CAI State Score', 'CAI Trait Full Score', 'STAI Trait Score']\n",
    "\n",
    "\n",
    "def get_combo_predictions(X0, slope_data):\n",
    "    corrs = []\n",
    "    ps = []\n",
    "    for y_col, y_i in zip(y_columns, range(len(y_columns))):\n",
    "        y0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        X = np.array([X0[0]])\n",
    "        y = np.array(y0[0])\n",
    "        \n",
    "        for i in range(1, len(X0)):  # remove NaN from data\n",
    "            is_nan = False\n",
    "            for x in X0[i]:\n",
    "                if math.isnan(x):\n",
    "                    is_nan = True\n",
    "                    break\n",
    "            if not math.isnan(y0[i]) and not is_nan:\n",
    "                X = np.append(X, [X0[i]], axis=0)\n",
    "                y = np.append(y, y0[i])\n",
    "        \n",
    "        \n",
    "        folds = min(10, len(X))\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(folds, shuffle=True, random_state=42)\n",
    "        predicted_vals0 = cross_val_predict(model, X, y, cv=cv)\n",
    "        actual_vals0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        #TODO: why this for actual_vals0 isntead of the sanitized y we already got?\n",
    "        predicted_vals = []\n",
    "        actual_vals = []\n",
    "        \n",
    "\n",
    "        for j in range(len(predicted_vals0)):\n",
    "            if not math.isnan(predicted_vals0[j]) and not math.isnan(actual_vals0[j]):\n",
    "                predicted_vals.append(predicted_vals0[j])\n",
    "                actual_vals.append(actual_vals0[j])\n",
    "\n",
    "        correlation, pval = pearsonr(predicted_vals, actual_vals)\n",
    "        #to_print = str(correlation) + str(pval)\n",
    "        #if pval < 0.15: \n",
    "        #    if not folds == 10:\n",
    "        #        print(\"With # KFolds\", folds)\n",
    "        #    print(\"{0}:\\t\\t{1}\\t\\t{2}\\t{3}\\t\\t{4}\".format(y_col, correlation, pval, \"Rows:\", len(y)))\n",
    "        corrs.append(correlation)\n",
    "        ps.append(pval)\n",
    "        \n",
    "    return corrs, ps\n",
    "\n",
    "def get_predictions(X0, slope_data):\n",
    "    corrs = []\n",
    "    ps = []\n",
    "    for y_col, y_i in zip(y_columns, range(len(y_columns))):\n",
    "        y0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        X = np.array(X0[0])\n",
    "        y = np.array(y0[0])\n",
    "        \n",
    "        for i in range(1,len(X0)):  # remove NaN from data\n",
    "            if math.isnan(y0[i]) or math.isnan(X0[i]):\n",
    "                continue\n",
    "            else:\n",
    "                X = np.append(X, X0[i])\n",
    "                y = np.append(y, y0[i])\n",
    "            \n",
    "        X = X.reshape(-1, 1)\n",
    "        folds = min(10, len(X))\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(folds, shuffle=True, random_state=42)\n",
    "        predicted_vals0 = cross_val_predict(model, X, y, cv=cv)\n",
    "        actual_vals0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        predicted_vals = []\n",
    "        actual_vals = []\n",
    "        \n",
    "\n",
    "        for j in range(len(predicted_vals0)):\n",
    "            if not math.isnan(predicted_vals0[j]) and not math.isnan(actual_vals0[j]):\n",
    "                predicted_vals.append(predicted_vals0[j])\n",
    "                actual_vals.append(actual_vals0[j])\n",
    "\n",
    "        correlation, pval = pearsonr(predicted_vals, actual_vals)\n",
    "        corrs.append(correlation)\n",
    "        ps.append(pval)\n",
    "        \n",
    "    return corrs, ps\n",
    "      \n",
    "def generate_combos(slope_data):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_titles = []\n",
    "    \n",
    "    demo_columns = [slope_data.columns[36], slope_data.columns[38], slope_data.columns[41], slope_data.columns[43]]\n",
    "    \n",
    "    group_titles.append('all demos')\n",
    "    X0_demo = slope_data[demo_columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_demo, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append(\"bios + demos\")\n",
    "    columns = slope_data.columns[25:33].to_list() + demo_columns\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('mfcc1-12 + demos')\n",
    "    columns = slope_data.columns[2:14].to_list() + demo_columns\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    X0_mfcc = slope_data[slope_data.columns[2:14]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    X0_pauses = slope_data[slope_data.columns[17:20]].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    X0_jitter = slope_data[slope_data.columns[22:24]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('jitter, shimmer, pauses')\n",
    "    columns = slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + jitter, shimmer')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('mfccs + jitter shimmer + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    X0_eda = slope_data[slope_data.columns[25:33]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('bio + jitter, shimmer')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[22:24].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('bio + jitter, shimmer + pauses')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('bio + pauses')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[23:30].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('all of em')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[25:33].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + demo_columns\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    return group_c, group_p, group_titles\n",
    "\n",
    "\n",
    "def combos_with_demo(slope_data, demo_col):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_titles = []\n",
    "    \n",
    "    #demo_columns = [slope_data.columns[36], slope_data.columns[38], slope_data.columns[41], slope_data.columns[43]]\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    columns = slope_data.columns[2:14].to_list() + [demo_col]\n",
    "    X0_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    columns = slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    columns = slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('jitter, shimmer, pauses')\n",
    "    columns = slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + jitter, shimmer')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('mfccs + jitter shimmer + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    columns = slope_data.columns[25:33].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('bio + jitter, shimmer')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('bio + jitter, shimmer + pauses')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('bio + pauses')\n",
    "    columns = slope_data.columns[25:33].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[25:33].to_list() + [demo_col]\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('all of em')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[25:33].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    return group_c, group_p, group_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def generate_graphs(c, p, titles, specifier):\n",
    "    short_y_col = [\"CAI St(19)\", \"CAI F(18)\" , \"STAI T(17)\"]\n",
    "    correlations = np.matrix(c)\n",
    "    pvalues = np.matrix(p)\n",
    "\n",
    "    correlations = np.round(correlations, decimals=2)\n",
    "    pvalues = np.round(pvalues, decimals=2)\n",
    "    \n",
    "    data = [correlations, pvalues]\n",
    "    graph_titles = [\"correlations_\" + specifier + \".png\", \"pvalues_\" + specifier + \".png\"]\n",
    "    \n",
    "    for k in range(2):\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,85))\n",
    "        \n",
    "        if k == 0:\n",
    "            p = ax.pcolor(data[k], vmin=-0.5, vmax=0.8)\n",
    "        else:\n",
    "            p = ax.pcolor(data[k], vmin = 0.0, vmax = 1.0)\n",
    "        #p = ax.matshow(data[k])\n",
    "        fig.colorbar(p, ax=ax, fraction=0.05, pad=0.04)\n",
    "        ax.set_xticklabels(labels=short_y_col)\n",
    "        ax.set_yticklabels(labels=titles)\n",
    "        plt.yticks(np.arange(0, len(titles), 1.0))\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(30)  \n",
    "\n",
    "        for i in range(len(short_y_col)):\n",
    "            for j in range(len(titles)):\n",
    "                text = ax.text(i + 0.5, j + 0.5, data[k][j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(DATA_PATH + 'slimmed/' + graph_titles[k])\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(36,44):\n",
    "    for j in range(i+1, 44):\n",
    "        c, p, titles = combos_with_demo(all_slopes, all_slopes.columns[i], all_slopes.columns[j])\n",
    "        filename = \"with-\" + str(all_slopes.columns[i]) + '-' + str(all_slopes.columns[j])\n",
    "        generate_graphs(c, p, titles, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, p1, titles1 = combos_with_demo(all_slopes, \"highest_education\")\n",
    "#generate_graphs(c, p, titles, \"with-highestedu\")\n",
    "c2, p2, titles2 = combos_with_demo(all_slopes, \"highest_education\")\n",
    "#generate_graphs(c, p, titles, \"with-highestedu\")\n",
    "c3, p3, titles3 = combos_with_demo(all_slopes, \"age\")\n",
    "#generate_graphs(c, p, titles, \"with-highestedu\")\n",
    "\n",
    "\n",
    "short_y_col = [\"CAI St(19)\", \"CAI F(18)\" , \"STAI T(17)\"]\n",
    "correlations = np.matrix(c)\n",
    "pvalues = np.matrix(p)\n",
    "\n",
    "correlations = np.round(correlations, decimals=2)\n",
    "pvalues = np.round(pvalues, decimals=2)\n",
    "    \n",
    "data = [correlations, pvalues]\n",
    "graph_titles = [\"correlations_\" + specifier + \".png\", \"pvalues_\" + specifier + \".png\"]\n",
    "    \n",
    "for k in range(2):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,85))\n",
    "        \n",
    "    if k == 0:\n",
    "        p = ax.pcolor(data[k], vmin=-0.5, vmax=0.8)\n",
    "    else:\n",
    "        p = ax.pcolor(data[k], vmin = 0.0, vmax = 1.0)\n",
    "    #p = ax.matshow(data[k])\n",
    "    fig.colorbar(p, ax=ax, fraction=0.05, pad=0.04)\n",
    "    ax.set_xticklabels(labels=short_y_col)\n",
    "    ax.set_yticklabels(labels=titles)\n",
    "    plt.yticks(np.arange(0, len(titles), 1.0))\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(30)  \n",
    "\n",
    "    for i in range(len(short_y_col)):\n",
    "        for j in range(len(titles)):\n",
    "            text = ax.text(i + 0.5, j + 0.5, data[k][j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(DATA_PATH + 'slimmed/' + graph_titles[k])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate graphs showing how each variable changes over time with regards to each of the survey measures\n",
    "correlations = []\n",
    "pvalues = []\n",
    "\n",
    "feature_sets = ['all demos', 'bios + demos', 'mfcc1-12 + demos', 'mfcc1-12', 'pauses', 'mfccs + pauses', 'jitter, shimmer', 'jitter, shimmer, pauses', 'mfccs + jitter, shimmer', 'mfccs + jitter shimmer + pauses', 'all bio', 'bio + jitter, shimmer', 'bio + jitter, shimmer + pauses', 'bio + pauses', 'bio + mfcc', 'everything']\n",
    "demo_feature_set = ['mfcc1-12', 'pauses', 'mfccs + pauses', 'jitter, shimmer', 'jitter, shimmer, pauses', 'mfccs + jitter, shimmer', 'mfccs + jitter shimmer + pauses', 'all bio', 'bio + jitter, shimmer', 'bio + jitter, shimmer + pauses', 'bio + pauses', 'bio + mfcc', 'everything']\n",
    "\n",
    "for i in range(2, 9):\n",
    "    s = get_slopes(all_data, 1, i)\n",
    "    c, p, titles = generate_combos(s)\n",
    "    correlations.append(c)\n",
    "    pvalues.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,40])\n",
    "fig.subplots_adjust(hspace=0.45, wspace=0.3)\n",
    "for i in range(16):\n",
    "    feature = [corr_set[i] for corr_set in correlations]\n",
    "    cai_st = np.asarray([vals[0] for vals in feature])\n",
    "    cai_f = np.asarray([vals[1] for vals in feature])\n",
    "    stai_t = np.asarray([vals[2] for vals in feature])\n",
    "    num_trials = np.arange(2, 9)\n",
    "    \n",
    "    ax = fig.add_subplot(8, 2, i+1)\n",
    "    ax.set_ylim(-0.8, 0.8)\n",
    "    ax.plot(num_trials, cai_st, label='CAI St')\n",
    "    ax.plot(num_trials, cai_f, label='CAI Full')\n",
    "    ax.plot(num_trials, stai_t, label='STAI T')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.xlabel('Number of trials')\n",
    "    plt.title(feature_sets[i])\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig(DATA_PATH + 'slimmed/' + 'change-with-trials')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c, p, titles = generate_combos(all_slopes)\n",
    "#generate_graphs(c, p, titles, \"general\")\n",
    "demo_columns = [all_slopes.columns[36], all_slopes.columns[38], all_slopes.columns[41], all_slopes.columns[43]]\n",
    "#demo_columns = [36, 38, 41, 43]\n",
    "\n",
    "for i in range(2, 9):\n",
    "    #for col in demo_columns:\n",
    "    graph_title = 'upto' + str(i)\n",
    "    s = get_slopes(all_data, 1, i)\n",
    "    c, p, titles = generate_combos(s)\n",
    "    generate_graphs(c, p, titles, graph_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2669699485347888, -0.10561382203579955, 0.3532157530697492], [-0.070999487182648, 0.15275545397482257, -0.2005726832429313], [-0.2855662035671647, 0.15113361580153292, 0.23577095560429634], [0.053493815038209394, 0.047223507493319036, 0.22767230632563432], [-0.22665433877601773, 0.3053959271737175, -0.24004330084141506], [0.33296709567800276, 0.09151383118278585, 0.37516795934671504], [-0.49207888130903454, -0.07149981336395966, 0.06662694826285616], [-0.2052413119015178, 0.16644591887022017, -0.08874598700288047], [0.04814414784074704, 0.28732080954997413, 0.15855334204448301], [0.5685544312713146, 0.2411248684416856, 0.362529508717691], [0.0008936851780006064, -0.45759808950974734, -0.4082464161068971], [-0.0059067585910285774, -0.37553816468412393, 0.1169025436613977], [-0.11361352780253507, -0.08470271208697285, 0.26047296078997295], [-0.11239419228500543, -0.14252235051321235, -0.20891834198247455], [-0.01923959051216894, -0.4942898710995437, 0.05690450227644874], [0.09410179904335264, 0.1592835843438625, 0.07031545132895645]]\n"
     ]
    }
   ],
   "source": [
    "c, p, titles = generate_combos(all_slopes)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
