{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.179487\n",
       "1     -0.179487\n",
       "2     -0.179487\n",
       "3     -0.179487\n",
       "4     -0.179487\n",
       "         ...   \n",
       "147    0.178571\n",
       "148    0.178571\n",
       "149    0.178571\n",
       "150    0.178571\n",
       "151    0.178571\n",
       "Name: CAI State Score, Length: 152, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "#############################\n",
    "# Import and combine all data\n",
    "#############################\n",
    "\n",
    "\n",
    "DATA_PATH = \"/Users/mvonebers/HUBBS-Lab/data/\"\n",
    "#DATA_PATH = \"/home/maggie/HUBBS-Lab/data/\"\n",
    "\n",
    "e4_data = pd.read_excel(DATA_PATH + \"E4_TEST.xlsx\")\n",
    "change_data = pd.read_excel(DATA_PATH + \"normalized_change.xlsx\")\n",
    "audio_data = pd.read_excel(DATA_PATH + \"audio_TEST.xlsx\")\n",
    "demo_data = pd.read_csv(DATA_PATH + \"Demographics Information.csv\")\n",
    "\n",
    "\n",
    "# Break apart the ID column into \"person\" and \"trial\"\n",
    "def clean_id(data):\n",
    "    data.insert(0, \"person\", [0] * data.shape[0])\n",
    "    data.insert(1, \"trial\", [0] * data.shape[0])\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        data.at[i, \"person\"] = int(data.at[i, \"id\"][7:])\n",
    "        data.at[i, \"trial\"] = int(data.at[i, \"id\"][5])\n",
    "    \n",
    "    data = data.drop(columns=['id'])\n",
    "    data = data.rename(columns={\"person\": \"id\"})\n",
    "    return data\n",
    "\n",
    "    \n",
    "e4_data = clean_id(e4_data)\n",
    "audio_data = clean_id(audio_data)    \n",
    "    \n",
    "all_data = pd.merge(e4_data, change_data, on='id')\n",
    "all_data = audio_data.merge(all_data, how='right')\n",
    "\n",
    "\n",
    "# Reorder survey data in order of most samples to least\n",
    "columns = all_data.columns.to_list()\n",
    "new_columns = deepcopy(columns)\n",
    "new_columns[35] = columns[40]\n",
    "new_columns[37] = columns[41] \n",
    "new_columns[38] = columns[37] \n",
    "new_columns[40] = columns[35]\n",
    "new_columns[41] = columns[38]\n",
    "\n",
    "all_data = all_data[new_columns]\n",
    "\n",
    "\n",
    "# what does this do?\n",
    "#demo_ids = demo_data['id'].to_list()\n",
    "#\n",
    "#for id_ in demo_ids:\n",
    "#    if id_ not in slope_ids:\n",
    "#        demo_data = demo_data[demo_data.id != id_]\n",
    "        \n",
    "all_data = all_data.merge(demo_data, how=\"right\")\n",
    "\n",
    "# The demographic data lists IDs that aren't present in the other data, so remove them\n",
    "all_data = all_data[all_data.id != 16]\n",
    "all_data = all_data[all_data.id != 27]\n",
    "all_data = all_data[all_data.id != 38]\n",
    "all_data = all_data[all_data.id != 43]\n",
    "all_data = all_data[all_data.id != 46]\n",
    "all_data = all_data[all_data.id != 49]\n",
    "all_data = all_data[all_data.id != 53]\n",
    "all_data = all_data[all_data.id != 58]\n",
    "all_data = all_data[all_data.id != 65]\n",
    "all_data = all_data[all_data.id != 66]\n",
    "\n",
    "all_data.drop(['CAI Trait Small group Score', 'CAI Trait Dyadic Score', 'CAI Trait Public Speaking Score', 'STAI State Score', 'Brief fear of Negative Evaluation'], axis=1, inplace=True)\n",
    "all_data['CAI State Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Get slopes from linear regression of the 8 trials for each ID\n",
    "###############################################################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def get_slopes(data, start, end):\n",
    "    y0 = data['trial'].to_numpy(copy=True)\n",
    "\n",
    "    slopes = pd.DataFrame(np.zeros((19, 45)), columns=data.columns)\n",
    "    slopes = slopes.drop([\"trial\"], axis=1)\n",
    "\n",
    "    for col in range(2, 34):\n",
    "        x1 = data[data.columns[col]]\n",
    "        y0 = list(range(start, end + 1))\n",
    "        for row in range(19):\n",
    "            x0 = x1[ (row * 8) + start - 1 : (row * 8) + end ].to_numpy()\n",
    "            x = np.array([])\n",
    "            y = np.array([])\n",
    "            \n",
    "            slopes.iloc[row, 0] = data.iloc[row * 8, 0]\n",
    "\n",
    "            for i in range(len(x0)):  # remove NaN from data\n",
    "                if not math.isnan(x0[i]) and not math.isnan(y0[i]):\n",
    "                    x = np.append(x, x0[i])\n",
    "                    y = np.append(y, y0[i])\n",
    "            \n",
    "            try:\n",
    "                reg = LinearRegression().fit(y.reshape(-1,1),x)\n",
    "                slopes.iloc[row, col - 1] = reg.coef_\n",
    "            except:\n",
    "                 slopes.iloc[row, col - 1] = 0\n",
    "    \n",
    "    for col in range(34, 45):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] = data.iloc[row * 8, col]\n",
    "            \n",
    "    # Want to preserve zeros in the demographic data, so temporarily boost it up one...\n",
    "    for col in range(43, 45):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] += 1.0\n",
    "                    \n",
    "    slopes.replace(0, np.NaN, inplace=True)\n",
    "    \n",
    "    # Then bump it back down. (I know this is a dumb way to do this.)\n",
    "    for col in range(43, 45):\n",
    "        for row in range(19):\n",
    "            slopes.iloc[row, col - 1] -= 1.0\n",
    "    \n",
    "    return slopes\n",
    "\n",
    "all_slopes = get_slopes(all_data, 1, 8)\n",
    "slope_ids = all_slopes['id'].to_list()\n",
    "#slope_ids\n",
    "all_slopes\n",
    "\n",
    "print(all_slopes['ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Problem: CAI State values are showing up as the CAI Dyadic...\n",
    "\n",
    "y_columns = ['CAI State Score', 'CAI Trait Full Score', 'STAI Trait Score']\n",
    "\n",
    "\n",
    "def get_combo_predictions(X0, slope_data):\n",
    "    corrs = []\n",
    "    ps = []\n",
    "    for y_col, y_i in zip(y_columns, range(len(y_columns))):\n",
    "        y0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        X = np.array([X0[0]])\n",
    "        y = np.array(y0[0])\n",
    "        \n",
    "        for i in range(1, len(X0)):  # remove NaN from data\n",
    "            is_nan = False\n",
    "            for x in X0[i]:\n",
    "                if math.isnan(x):\n",
    "                    is_nan = True\n",
    "                    break\n",
    "            if not math.isnan(y0[i]) and not is_nan:\n",
    "                X = np.append(X, [X0[i]], axis=0)\n",
    "                y = np.append(y, y0[i])\n",
    "        \n",
    "        \n",
    "        folds = min(10, len(X))\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(folds, shuffle=True, random_state=42)\n",
    "        predicted_vals0 = cross_val_predict(model, X, y, cv=cv)\n",
    "        actual_vals0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        #TODO: why this for actual_vals0 isntead of the sanitized y we already got?\n",
    "        predicted_vals = []\n",
    "        actual_vals = []\n",
    "        \n",
    "\n",
    "        for j in range(len(predicted_vals0)):\n",
    "            if not math.isnan(predicted_vals0[j]) and not math.isnan(actual_vals0[j]):\n",
    "                predicted_vals.append(predicted_vals0[j])\n",
    "                actual_vals.append(actual_vals0[j])\n",
    "\n",
    "        correlation, pval = pearsonr(predicted_vals, actual_vals)\n",
    "        #to_print = str(correlation) + str(pval)\n",
    "        #if pval < 0.15: \n",
    "        #    if not folds == 10:\n",
    "        #        print(\"With # KFolds\", folds)\n",
    "        #    print(\"{0}:\\t\\t{1}\\t\\t{2}\\t{3}\\t\\t{4}\".format(y_col, correlation, pval, \"Rows:\", len(y)))\n",
    "        corrs.append(correlation)\n",
    "        ps.append(pval)\n",
    "        \n",
    "    return corrs, ps\n",
    "\n",
    "def get_predictions(X0, slope_data):\n",
    "    corrs = []\n",
    "    ps = []\n",
    "    for y_col, y_i in zip(y_columns, range(len(y_columns))):\n",
    "        y0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        X = np.array(X0[0])\n",
    "        y = np.array(y0[0])\n",
    "        \n",
    "        for i in range(1,len(X0)):  # remove NaN from data\n",
    "            if math.isnan(y0[i]) or math.isnan(X0[i]):\n",
    "                continue\n",
    "            else:\n",
    "                X = np.append(X, X0[i])\n",
    "                y = np.append(y, y0[i])\n",
    "            \n",
    "        X = X.reshape(-1, 1)\n",
    "        folds = min(10, len(X))\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(folds, shuffle=True, random_state=42)\n",
    "        predicted_vals0 = cross_val_predict(model, X, y, cv=cv)\n",
    "        actual_vals0 = slope_data[y_col].to_numpy(copy=True)\n",
    "        predicted_vals = []\n",
    "        actual_vals = []\n",
    "        \n",
    "\n",
    "        for j in range(len(predicted_vals0)):\n",
    "            if not math.isnan(predicted_vals0[j]) and not math.isnan(actual_vals0[j]):\n",
    "                predicted_vals.append(predicted_vals0[j])\n",
    "                actual_vals.append(actual_vals0[j])\n",
    "\n",
    "        correlation, pval = pearsonr(predicted_vals, actual_vals)\n",
    "        corrs.append(correlation)\n",
    "        ps.append(pval)\n",
    "        \n",
    "    return corrs, ps\n",
    "      \n",
    "def generate_combos(slope_data):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_titles = []\n",
    "    \n",
    "    demo_columns = [slope_data.columns[36], slope_data.columns[38], slope_data.columns[41], slope_data.columns[43]]\n",
    "    \n",
    "    group_titles.append('all demos')\n",
    "    X0_demo = slope_data[demo_columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_demo, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append(\"bios + demos\")\n",
    "    columns = slope_data.columns[23:30].to_list() + demo_columns\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('mfcc1-12 + demos')\n",
    "    columns = slope_data.columns[2:14].to_list() + demo_columns\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    X0_mfcc = slope_data[slope_data.columns[2:14]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    X0_pauses = slope_data[slope_data.columns[17:20]].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    X0_jitter = slope_data[slope_data.columns[22:24]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('jitter, shimmer, pauses')\n",
    "    columns = slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + jitter, shimmer')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('mfccs + jitter shimmer + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list()\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    X0_eda = slope_data[slope_data.columns[23:30]].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list()\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('all of em')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + demo_columns\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    return group_c, group_p, group_titles\n",
    "\n",
    "\n",
    "def combos_with_demo(slope_data, demo_col):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_titles = []\n",
    "    \n",
    "    #demo_columns = [slope_data.columns[36], slope_data.columns[38], slope_data.columns[41], slope_data.columns[43]]\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    columns = slope_data.columns[2:14].to_list() + [demo_col]\n",
    "    X0_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    columns = slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    columns = slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('jitter, shimmer, pauses')\n",
    "    columns = slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + jitter, shimmer')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('mfccs + jitter shimmer + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    columns = slope_data.columns[23:30].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list() + [demo_col]\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    group_titles.append('all of em')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list() + slope_data.columns[22:24].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0 = slope_data[columns].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    return group_c, group_p, group_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def generate_graphs(c, p, titles, specifier):\n",
    "    short_y_col = [\"CAI St(19)\", \"CAI F(18)\" , \"STAI T(17)\"]\n",
    "    correlations = np.matrix(c)\n",
    "    pvalues = np.matrix(p)\n",
    "\n",
    "    correlations = np.round(correlations, decimals=2)\n",
    "    pvalues = np.round(pvalues, decimals=2)\n",
    "    \n",
    "    data = [correlations, pvalues]\n",
    "    graph_titles = [\"correlations_\" + specifier + \".png\", \"pvalues_\" + specifier + \".png\"]\n",
    "    \n",
    "    for k in range(2):\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,85))\n",
    "        \n",
    "        if k == 0:\n",
    "            p = ax.pcolor(data[k], vmin=-0.5, vmax=0.8)\n",
    "        else:\n",
    "            p = ax.pcolor(data[k], vmin = 0.0, vmax = 1.0)\n",
    "        #p = ax.matshow(data[k])\n",
    "        fig.colorbar(p, ax=ax, fraction=0.05, pad=0.04)\n",
    "        ax.set_xticklabels(labels=short_y_col)\n",
    "        ax.set_yticklabels(labels=titles)\n",
    "        plt.yticks(np.arange(0, len(titles), 1.0))\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(30)  \n",
    "\n",
    "        for i in range(len(short_y_col)):\n",
    "            for j in range(len(titles)):\n",
    "                text = ax.text(i + 0.5, j + 0.5, data[k][j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(DATA_PATH + 'slimmed/' + graph_titles[k])\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combos_with_demo(slope_data, demo_col):\n",
    "    group_c = []\n",
    "    group_p = []\n",
    "    group_titles = []\n",
    "    \n",
    "    group_titles.append('voiceProb_sma_amean')\n",
    "    X0 = slopes_data['voiceProb_sma_amean'].to_numpy(copy=True)\n",
    "    c, p = get_combo_predictions(X0, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfcc1-12')\n",
    "    columns = slope_data.columns[2:14].to_list() + [demo_col]\n",
    "    X0_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('pauses')\n",
    "    columns = slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs + pauses')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[17:20].to_list() + [demo_col]\n",
    "    X0_pauses = slope_data[columns].to_numpy(copy=True) # #pause, pause_frequency, pause_interval\n",
    "    c, p = get_combo_predictions(X0_pauses, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('jitter, shimmer')\n",
    "    columns = slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('mfccs+ jitter, shimmer')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[22:24].to_list() + [demo_col]\n",
    "    X0_jitter = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_jitter, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append(\"all bio\")\n",
    "    columns = slope_data.columns[23:30].to_list() + [demo_col]\n",
    "    X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "\n",
    "    group_titles.append('bio + mfcc')\n",
    "    columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list() + [demo_col]\n",
    "    X0_bio_mfcc = slope_data[columns].to_numpy(copy=True)\n",
    "    c,p = get_combo_predictions(X0_bio_mfcc, slope_data)\n",
    "    group_c.append(c)\n",
    "    group_p.append(p)\n",
    "    \n",
    "    #return group_c, group_p, group_titles\n",
    "\n",
    "    # All mfcc's combined with other attributes\n",
    "    for i in range(14, 25):\n",
    "        title = \"mfccs + \" + str(slope_data.columns[i])\n",
    "        group_titles.append(title)\n",
    "        columns = slope_data.columns[2:14].to_list() + [slope_data.columns[i]] + [demo_col]\n",
    "        X0 = slope_data[columns].to_numpy(copy=True)\n",
    "        c,p = get_combo_predictions(X0, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "\n",
    "    for i in range(14, 25):\n",
    "        col1 = slope_data.columns[i]\n",
    "        for j in range(i + 1, 25):\n",
    "            col2 = slope_data.columns[j]\n",
    "            title = str(col1) + '+' + str(col2)\n",
    "            group_titles.append(title)\n",
    "            X0 = slope_data[[col1, col2, demo_col]].to_numpy(copy=True)\n",
    "            c, p = get_combo_predictions(X0, slope_data)\n",
    "            group_c.append(c)\n",
    "            group_p.append(p)\n",
    "\n",
    "    for i in range(14, 25):\n",
    "        col1 = slope_data.columns[i]\n",
    "        for j in range(i + 1, 25):\n",
    "            col2 = slope_data.columns[j]\n",
    "            columns = slope_data.columns[2:14].to_list() + [col1] + [col2] + [demo_col]\n",
    "            title = \"mfccs + \" + str(col1) + '+' + str(col2)\n",
    "            group_titles.append(title)\n",
    "            X0 = slope_data[[col1, col2]].to_numpy(copy=True)\n",
    "            c, p = get_combo_predictions(X0, slope_data)\n",
    "            group_c.append(c)\n",
    "            group_p.append(p)\n",
    "\n",
    "    for i in range(14, 24):\n",
    "        col = slope_data.columns[i]\n",
    "        title = 'bio +' + str(col)\n",
    "        group_titles.append(title)\n",
    "        columns = slope_data.columns[23:30].to_list() + [col] + [demo_col]\n",
    "        X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "        c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "\n",
    "    for i in range(14, 24):\n",
    "        col = slope_data.columns[i]\n",
    "        title = 'mfcc + bio +' + str(col)\n",
    "        group_titles.append(title)\n",
    "        columns = slope_data.columns[2:14].to_list() + slope_data.columns[23:30].to_list() + [col] + [demo_col]\n",
    "        X0_eda = slope_data[columns].to_numpy(copy=True)\n",
    "        c,p = get_combo_predictions(X0_eda, slope_data)\n",
    "        group_c.append(c)\n",
    "        group_p.append(p)\n",
    "        \n",
    "    return group_c, group_p, group_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(36,44):\n",
    "    for j in range(i+1, 44):\n",
    "        c, p, titles = combos_with_demo(all_slopes, all_slopes.columns[i], all_slopes.columns[j])\n",
    "        filename = \"with-\" + str(all_slopes.columns[i]) + '-' + str(all_slopes.columns[j])\n",
    "        generate_graphs(c, p, titles, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, p1, titles1 = combos_with_demo(all_slopes, \"highest_education\")\n",
    "#generate_graphs(c, p, titles, \"with-highestedu\")\n",
    "c2, p2, titles2 = combos_with_demo(all_slopes, \"highest_education\")\n",
    "#generate_graphs(c, p, titles, \"with-highestedu\")\n",
    "c3, p3, titles3 = combos_with_demo(all_slopes, \"age\")\n",
    "#generate_graphs(c, p, titles, \"with-highestedu\")\n",
    "\n",
    "\n",
    "short_y_col = [\"CAI St(19)\", \"CAI F(18)\" , \"STAI T(17)\"]\n",
    "correlations = np.matrix(c)\n",
    "pvalues = np.matrix(p)\n",
    "\n",
    "correlations = np.round(correlations, decimals=2)\n",
    "pvalues = np.round(pvalues, decimals=2)\n",
    "    \n",
    "data = [correlations, pvalues]\n",
    "graph_titles = [\"correlations_\" + specifier + \".png\", \"pvalues_\" + specifier + \".png\"]\n",
    "    \n",
    "for k in range(2):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,85))\n",
    "        \n",
    "    if k == 0:\n",
    "        p = ax.pcolor(data[k], vmin=-0.5, vmax=0.8)\n",
    "    else:\n",
    "        p = ax.pcolor(data[k], vmin = 0.0, vmax = 1.0)\n",
    "    #p = ax.matshow(data[k])\n",
    "    fig.colorbar(p, ax=ax, fraction=0.05, pad=0.04)\n",
    "    ax.set_xticklabels(labels=short_y_col)\n",
    "    ax.set_yticklabels(labels=titles)\n",
    "    plt.yticks(np.arange(0, len(titles), 1.0))\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(30)  \n",
    "\n",
    "    for i in range(len(short_y_col)):\n",
    "        for j in range(len(titles)):\n",
    "            text = ax.text(i + 0.5, j + 0.5, data[k][j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(DATA_PATH + 'slimmed/' + graph_titles[k])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
